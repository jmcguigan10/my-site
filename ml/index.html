<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Machine Learning Project</title>
    <link rel="icon" type="image/png" href="../favicon.png">
    <link rel="stylesheet" href="../assets/styles.css">
  </head>
  <body>
    <header class="site-header">
      <div class="wrap">
        <a class="brand" href="../index.html">JM</a>
        <nav class="nav">
          <a href="../index.html">Home</a>
          <a href="../tools/filemaker/">Tool</a>
          <a class="active" href="./">ML Project</a>
          <a href="../resume.pdf">Resume</a>
        </nav>
      </div>
    </header>
    <main class="wrap">
      <section class="hero slim">
        <h1>Machine Learning Project</h1>
        <p class="lede">Clean, readable source with a simple navigator. Use the sidebar to jump to files.</p>
        <div class="cta-row">
          <a class="btn primary" href="./ntrno/">FFI Stability Classifier</a>
        </div>
      </section>
      <section class="layout">
        <aside class="sidebar" id="fileList"></aside>
        <article class="content">
          <div id="viewer" class="viewer">
            <p>Select a file from the left to view its contents.</p>
          </div>
        </article>
      </section>
    </main>
    <footer class="site-footer">
      <div class="wrap">
        <p>Â© <span id="year"></span> JM. All rights reserved.</p>
      </div>
    </footer>
    <script>
      const manifest = [{"path": "README.md", "size": 2484, "textual": true, "content": "**Docs hub:** [docs/index.md](docs/index.md) \u00b7 [Site Map](docs/site-map.md) \u00b7 [Code Browser](docs/code-browser.md)\n\n\n> Navigation: [Home](docs/index.md) \u00b7 [Site Map](docs/site-map.md) \u00b7 [Code Browser](docs/code-browser.md)\n\n# ML Optimization Project\n\nA clean, modular scaffold carved out of a single-file script. Toggle between learning algorithms via YAML. Run experiments without the spaghetti.\n\n## TL;DR\n\n1. Choose a config in `configs/` (e.g. `configs/adam.yaml`).\n2. Run:\n   ```bash\n   python scripts/run_training.py --config configs/adam.yaml\n   ```\n\nYour original file has been preserved at `src/training/legacy/grad_desc_prob.py`.\n\n## Directory tree\n\n```text\n.\n\u251c\u2500\u2500 configs/\n\u2502   \u251c\u2500\u2500 default.yaml\n\u2502   \u251c\u2500\u2500 sgd.yaml\n\u2502   \u251c\u2500\u2500 momentum.yaml\n\u2502   \u2514\u2500\u2500 adam.yaml\n``` \n[README](configs/README.md)\n```text\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 architecture.md\n\u2502   \u2514\u2500\u2500 directory-map.md\n``` \n- [Architecture](docs/architecture.md)\n- [Directory](docs/directory-map.md)\n- [Index](docs/index.md)\n```text\n\u251c\u2500\u2500 experiments/\n\u2502   \u2514\u2500\u2500 2025-10-20-baseline/\n\u2502       \u251c\u2500\u2500 notes.md\n\u2502       \u2514\u2500\u2500 .gitkeep\n``` \n[README](experiments/README.md)\n```text\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 run_training.py\n``` \n[README](scripts/README.md)\n```text\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 algorithms/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 sgd.py\n\u2502   \u2502   \u251c\u2500\u2500 momentum.py\n\u2502   \u2502   \u2514\u2500\u2500 adam.py\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 loaders.py\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 linear_regression.py\n\u2502   \u251c\u2500\u2500 training/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 train.py\n\u2502   \u2502   \u2514\u2500\u2500 legacy/\n\u2502   \u2502       \u2514\u2500\u2500 grad_desc_prob.py\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 config.py\n\u2502       \u2514\u2500\u2500 logging.py\n``` \n[README](src/README.md)\n```text\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_optimizers.py\n```\n\n## Add a new algorithm\n\n- Create a new file under `src/algorithms/your_algo.py` implementing a class with `.step(params, grads)` and an optional `.state` dict.\n- Add a new config file under `configs/your_algo.yaml` with `training.algorithm: \"your_algo\"`.\n- Run with `python scripts/run_training.py --config configs/your_algo.yaml`.\n\nSee `docs/architecture.md` for the Markdown link web and system diagram."}, {"path": "configs/momentum.yaml", "size": 362, "textual": true, "content": "experiment:\n  name: \"momentum-run\"\n  seed: 2025\n  output_dir: \"experiments/2025-10-20-momentum\"\ndata:\n  kind: \"synthetic_linear\"\n  n_samples: 1024\n  n_features: 5\n  noise_std: 0.1\nmodel:\n  type: \"linear_regression\"\ntraining:\n  algorithm: \"momentum\"\n  epochs: 150\n  lr: 0.03\n  momentum: 0.9\n  batch_size: 64\nlogging:\n  level: \"INFO\"\n  log_dir: \"experiments/logs\"\n"}, {"path": "configs/adam.yaml", "size": 373, "textual": true, "content": "experiment:\n  name: \"adam-run\"\n  seed: 7\n  output_dir: \"experiments/2025-10-20-adam\"\ndata:\n  kind: \"synthetic_linear\"\n  n_samples: 2048\n  n_features: 5\n  noise_std: 0.1\nmodel:\n  type: \"linear_regression\"\ntraining:\n  algorithm: \"adam\"\n  epochs: 120\n  lr: 0.01\n  beta1: 0.9\n  beta2: 0.999\n  eps: 1.0e-8\n  batch_size: 64\nlogging:\n  level: \"INFO\"\n  log_dir: \"experiments/logs\"\n"}, {"path": "configs/back-arrow.png", "size": 47253, "textual": false, "content": null}, {"path": "configs/README.md", "size": 949, "textual": true, "content": "> Navigation: [Home](../docs/index.md) \u00b7 [Site Map](../docs/site-map.md) \u00b7 [Code Browser](../docs/code-browser.md)\n\n<style>\n  .back-link {\n    position: fixed;       /* stay in viewport as you scroll */\n    top: 10px;              /* adjust vertical offset */\n    left: 10px;             /* adjust horizontal offset */\n    z-index: 1000;          /* sit above most stuff */\n  }\n\n  .back-link img {\n    width: 30px;             /* size of your arrow \u2014 change as needed */\n    height: auto;\n    cursor: pointer;\n  }\n</style>\n\n<a href=\"../README.md\" class=\"back-link\">\n  <img src=\"back-arrow.png\" alt=\"Back\">\n</a>\n\n## Configs\n\nPick one of these YAMLs when running `scripts/run_training.py`. The `training.algorithm` key selects which optimizer to use.\n\n- `default.yaml` minimal example\n- `sgd.yaml` stochastic gradient descent\n- `momentum.yaml` SGD with momentum\n- `adam.yaml` Adam optimizer\n\nYou can clone one and change hyperparameters as needed."}, {"path": "configs/sgd.yaml", "size": 336, "textual": true, "content": "experiment:\n  name: \"sgd-baseline\"\n  seed: 1337\n  output_dir: \"experiments/2025-10-20-sgd\"\ndata:\n  kind: \"synthetic_linear\"\n  n_samples: 1024\n  n_features: 5\n  noise_std: 0.1\nmodel:\n  type: \"linear_regression\"\ntraining:\n  algorithm: \"sgd\"\n  epochs: 150\n  lr: 0.05\n  batch_size: 64\nlogging:\n  level: \"INFO\"\n  log_dir: \"experiments/logs\"\n"}, {"path": "configs/default.yaml", "size": 368, "textual": true, "content": "experiment:\n  name: \"baseline\"\n  seed: 42\n  output_dir: \"experiments/2025-10-20-baseline\"\ndata:\n  kind: \"synthetic_linear\"\n  n_samples: 512\n  n_features: 3\n  noise_std: 0.1\nmodel:\n  type: \"linear_regression\"\ntraining:\n  algorithm: \"sgd\"  # choices: sgd | momentum | adam\n  epochs: 100\n  lr: 0.05\n  batch_size: 64\nlogging:\n  level: \"INFO\"\n  log_dir: \"experiments/logs\"\n"}, {"path": "docs/architecture.md", "size": 1416, "textual": true, "content": "> Navigation: [Home](index.md) \u00b7 [Site Map](site-map.md) \u00b7 [Code Browser](code-browser.md)\n\n<style>\n  .back-link {\n    position: fixed;       /* stay in viewport as you scroll */\n    top: 10px;              /* adjust vertical offset */\n    left: 10px;             /* adjust horizontal offset */\n    z-index: 1000;          /* sit above most stuff */\n  }\n\n  .back-link img {\n    width: 30px;             /* size of your arrow \u2014 change as needed */\n    height: auto;\n    cursor: pointer;\n  }\n</style>\n\n<a href=\"../README.md\" class=\"back-link\">\n  <img src=\"back-arrow.png\" alt=\"Back\">\n</a>\n\n# System Architecture\n\nThis markdown is the \"web\" of links between pieces. Start anywhere, follow the trails.\n\n- `configs/*.yaml` \u2192 drives `scripts/run_training.py`\n- `scripts/run_training.py` \u2192 loads YAML via `src/utils/config.py`, sets seeds and logging\n- `src/training/train.py` \u2192 orchestrates data \u2192 model \u2192 optimizer\n- `src/algorithms/*` \u2192 the optimizers selected by `training.algorithm`\n- `src/models/linear_regression.py` \u2192 the example model\n- `src/data/loaders.py` \u2192 synthetic data generator\n\n## Graph (Mermaid)\n\n```mermaid\ngraph TD\n  C[configs/*.yaml] --> R[scripts/run_training.py]\n  R --> U[src/utils/config.py]\n  R --> T[src/training/train.py]\n  T --> D[src/data/loaders.py]\n  T --> M[src/models/linear_regression.py]\n  T --> A[src/algorithms/*]\n```\n\n- See also: [Directory Map](directory-map.md)"}, {"path": "docs/back-arrow.png", "size": 47253, "textual": false, "content": null}, {"path": "docs/index.md", "size": 955, "textual": true, "content": "# Project Docs\n\n- \ud83d\uddfa\ufe0f [Site Map](site-map.md)\n- \ud83d\udcc1 [Code Browser](code-browser.md)\n\n> Navigation: [Home](index.md) \u00b7 [Site Map](site-map.md) \u00b7 [Code Browser](code-browser.md)\n\n<style>\n  .back-link {\n    position: fixed;       /* stay in viewport as you scroll */\n    top: 10px;              /* adjust vertical offset */\n    left: 10px;             /* adjust horizontal offset */\n    z-index: 1000;          /* sit above most stuff */\n  }\n\n  .back-link img {\n    width: 30px;             /* size of your arrow \u2014 change as needed */\n    height: auto;\n    cursor: pointer;\n  }\n</style>\n\n<a href=\"../README.md\" class=\"back-link\">\n  <img src=\"back-arrow.png\" alt=\"Back\">\n</a>\n\n# Docs\n\n- [Architecture](architecture.md)\n- [Directory Map](directory-map.md)\n- [Configs](../configs/README.md)\n- [Algorithms](../src/algorithms/README.md)\n- [Training Pipeline](../src/training/README.md)\n- [Models](../src/models/README.md)\n- [Data](../src/data/README.md)\n"}, {"path": "docs/directory-map.md", "size": 1156, "textual": true, "content": "> Navigation: [Home](index.md) \u00b7 [Site Map](site-map.md) \u00b7 [Code Browser](code-browser.md)\n\n<style>\n  .back-link {\n    position: fixed;       /* stay in viewport as you scroll */\n    top: 10px;              /* adjust vertical offset */\n    left: 10px;             /* adjust horizontal offset */\n    z-index: 1000;          /* sit above most stuff */\n  }\n\n  .back-link img {\n    width: 30px;             /* size of your arrow \u2014 change as needed */\n    height: auto;\n    cursor: pointer;\n  }\n</style>\n\n<a href=\"../README.md\" class=\"back-link\">\n  <img src=\"back-arrow.png\" alt=\"Back\">\n</a>\n\n# Directory Map\n\n```text\ndocs/\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 architecture.md\n\u2514\u2500\u2500 directory-map.md\n```\n\nThis repo uses small README files in each folder as \"nodes\" in a markdown web. Navigate them:\n\n- [configs](../configs/README.md)\n- [src](../src/README.md)\n  - [algorithms](../src/algorithms/README.md)\n  - [data](../src/data/README.md)\n  - [models](../src/models/README.md)\n  - [training](../src/training/README.md)\n  - [utils](../src/utils/README.md)\n- [scripts](../scripts/README.md)\n- [tests](../tests/README.md)\n- [experiments](../experiments/README.md)"}, {"path": "docs/code-browser.md", "size": 2254, "textual": true, "content": "# Code Browser\n\nNavigate the codebase with preview pages. Each row links to the real file and preview.\n\n## scripts\n\n| Path | Open file | Preview |\n|---|---|---|\n| `scripts/run_training.py` | [run_training.py](../scripts/run_training.py) | [preview](code/scripts/run_training.md) |\n\n## src\n\n| Path | Open file | Preview |\n|---|---|---|\n| `src/__init__.py` | [__init__.py](../src/__init__.py) | [preview](code/src/__init__.md) |\n| `src/algorithms/__init__.py` | [__init__.py](../src/algorithms/__init__.py) | [preview](code/src/algorithms/__init__.md) |\n| `src/algorithms/adam.py` | [adam.py](../src/algorithms/adam.py) | [preview](code/src/algorithms/adam.md) |\n| `src/algorithms/momentum.py` | [momentum.py](../src/algorithms/momentum.py) | [preview](code/src/algorithms/momentum.md) |\n| `src/algorithms/sgd.py` | [sgd.py](../src/algorithms/sgd.py) | [preview](code/src/algorithms/sgd.md) |\n| `src/data/__init__.py` | [__init__.py](../src/data/__init__.py) | [preview](code/src/data/__init__.md) |\n| `src/data/loaders.py` | [loaders.py](../src/data/loaders.py) | [preview](code/src/data/loaders.md) |\n| `src/models/__init__.py` | [__init__.py](../src/models/__init__.py) | [preview](code/src/models/__init__.md) |\n| `src/models/linear_regression.py` | [linear_regression.py](../src/models/linear_regression.py) | [preview](code/src/models/linear_regression.md) |\n| `src/training/__init__.py` | [__init__.py](../src/training/__init__.py) | [preview](code/src/training/__init__.md) |\n| `src/training/legacy/grad_desc_prob.py` | [grad_desc_prob.py](../src/training/legacy/grad_desc_prob.py) | [preview](code/src/training/legacy/grad_desc_prob.md) |\n| `src/training/train.py` | [train.py](../src/training/train.py) | [preview](code/src/training/train.md) |\n| `src/utils/__init__.py` | [__init__.py](../src/utils/__init__.py) | [preview](code/src/utils/__init__.md) |\n| `src/utils/config.py` | [config.py](../src/utils/config.py) | [preview](code/src/utils/config.md) |\n| `src/utils/logging.py` | [logging.py](../src/utils/logging.py) | [preview](code/src/utils/logging.md) |\n\n## tests\n\n| Path | Open file | Preview |\n|---|---|---|\n| `tests/test_optimizers.py` | [test_optimizers.py](../tests/test_optimizers.py) | [preview](code/tests/test_optimizers.md) |\n\n"}, {"path": "docs/site-map.md", "size": 6361, "textual": true, "content": "# Site Map\n\nFull-tree clickable map. Direct links for files, and preview links for code.\n\n- **[configs/](../configs/README.md)**\n  - `configs/adam.yaml` \u2192 [adam.yaml](../configs/adam.yaml)\n  - `configs/back-arrow.png` \u2192 [back-arrow.png](../configs/back-arrow.png)\n  - `configs/default.yaml` \u2192 [default.yaml](../configs/default.yaml)\n  - `configs/momentum.yaml` \u2192 [momentum.yaml](../configs/momentum.yaml)\n  - `configs/README.md` \u2192 [README.md](../configs/README.md)\n  - `configs/sgd.yaml` \u2192 [sgd.yaml](../configs/sgd.yaml)\n- **docs/**\n  - **docs/code/**\n    - **docs/code/scripts/**\n      - `docs/code/scripts/run_training.md` \u2192 [run_training.md](code/scripts/run_training.md)\n    - **docs/code/src/**\n      - **docs/code/src/algorithms/**\n        - `docs/code/src/algorithms/__init__.md` \u2192 [__init__.md](code/src/algorithms/__init__.md)\n        - `docs/code/src/algorithms/adam.md` \u2192 [adam.md](code/src/algorithms/adam.md)\n        - `docs/code/src/algorithms/momentum.md` \u2192 [momentum.md](code/src/algorithms/momentum.md)\n        - `docs/code/src/algorithms/sgd.md` \u2192 [sgd.md](code/src/algorithms/sgd.md)\n      - **docs/code/src/data/**\n        - `docs/code/src/data/__init__.md` \u2192 [__init__.md](code/src/data/__init__.md)\n        - `docs/code/src/data/loaders.md` \u2192 [loaders.md](code/src/data/loaders.md)\n      - **docs/code/src/models/**\n        - `docs/code/src/models/__init__.md` \u2192 [__init__.md](code/src/models/__init__.md)\n        - `docs/code/src/models/linear_regression.md` \u2192 [linear_regression.md](code/src/models/linear_regression.md)\n      - **docs/code/src/training/**\n        - **docs/code/src/training/legacy/**\n          - `docs/code/src/training/legacy/grad_desc_prob.md` \u2192 [grad_desc_prob.md](code/src/training/legacy/grad_desc_prob.md)\n        - `docs/code/src/training/__init__.md` \u2192 [__init__.md](code/src/training/__init__.md)\n        - `docs/code/src/training/train.md` \u2192 [train.md](code/src/training/train.md)\n      - **docs/code/src/utils/**\n        - `docs/code/src/utils/__init__.md` \u2192 [__init__.md](code/src/utils/__init__.md)\n        - `docs/code/src/utils/config.md` \u2192 [config.md](code/src/utils/config.md)\n        - `docs/code/src/utils/logging.md` \u2192 [logging.md](code/src/utils/logging.md)\n      - `docs/code/src/__init__.md` \u2192 [__init__.md](code/src/__init__.md)\n    - **docs/code/tests/**\n      - `docs/code/tests/test_optimizers.md` \u2192 [test_optimizers.md](code/tests/test_optimizers.md)\n  - `docs/architecture.md` \u2192 [architecture.md](architecture.md)\n  - `docs/back-arrow.png` \u2192 [back-arrow.png](back-arrow.png)\n  - `docs/code-browser.md` \u2192 [code-browser.md](code-browser.md)\n  - `docs/directory-map.md` \u2192 [directory-map.md](directory-map.md)\n  - `docs/index.md` \u2192 [index.md](index.md)\n- **[experiments/](../experiments/README.md)**\n  - **experiments/2025-10-20-baseline/**\n    - `experiments/2025-10-20-baseline/notes.md` \u2192 [notes.md](../experiments/2025-10-20-baseline/notes.md)\n  - `experiments/back-arrow.png` \u2192 [back-arrow.png](../experiments/back-arrow.png)\n  - `experiments/README.md` \u2192 [README.md](../experiments/README.md)\n- **[scripts/](../scripts/README.md)**\n  - `scripts/back-arrow.png` \u2192 [back-arrow.png](../scripts/back-arrow.png)\n  - `scripts/README.md` \u2192 [README.md](../scripts/README.md)\n  - `scripts/run_training.py` \u2192 [run_training.py](../scripts/run_training.py) \u00b7 [preview](code/scripts/run_training.md)\n- **[src/](../src/README.md)**\n  - **[src/algorithms/](../src/algorithms/README.md)**\n    - `src/algorithms/__init__.py` \u2192 [__init__.py](../src/algorithms/__init__.py) \u00b7 [preview](code/src/algorithms/__init__.md)\n    - `src/algorithms/adam.py` \u2192 [adam.py](../src/algorithms/adam.py) \u00b7 [preview](code/src/algorithms/adam.md)\n    - `src/algorithms/momentum.py` \u2192 [momentum.py](../src/algorithms/momentum.py) \u00b7 [preview](code/src/algorithms/momentum.md)\n    - `src/algorithms/README.md` \u2192 [README.md](../src/algorithms/README.md)\n    - `src/algorithms/sgd.py` \u2192 [sgd.py](../src/algorithms/sgd.py) \u00b7 [preview](code/src/algorithms/sgd.md)\n  - **[src/data/](../src/data/README.md)**\n    - `src/data/__init__.py` \u2192 [__init__.py](../src/data/__init__.py) \u00b7 [preview](code/src/data/__init__.md)\n    - `src/data/loaders.py` \u2192 [loaders.py](../src/data/loaders.py) \u00b7 [preview](code/src/data/loaders.md)\n    - `src/data/README.md` \u2192 [README.md](../src/data/README.md)\n  - **[src/models/](../src/models/README.md)**\n    - `src/models/__init__.py` \u2192 [__init__.py](../src/models/__init__.py) \u00b7 [preview](code/src/models/__init__.md)\n    - `src/models/linear_regression.py` \u2192 [linear_regression.py](../src/models/linear_regression.py) \u00b7 [preview](code/src/models/linear_regression.md)\n    - `src/models/README.md` \u2192 [README.md](../src/models/README.md)\n  - **[src/training/](../src/training/README.md)**\n    - **src/training/legacy/**\n      - `src/training/legacy/grad_desc_prob.py` \u2192 [grad_desc_prob.py](../src/training/legacy/grad_desc_prob.py) \u00b7 [preview](code/src/training/legacy/grad_desc_prob.md)\n    - `src/training/__init__.py` \u2192 [__init__.py](../src/training/__init__.py) \u00b7 [preview](code/src/training/__init__.md)\n    - `src/training/README.md` \u2192 [README.md](../src/training/README.md)\n    - `src/training/train.py` \u2192 [train.py](../src/training/train.py) \u00b7 [preview](code/src/training/train.md)\n  - **[src/utils/](../src/utils/README.md)**\n    - `src/utils/__init__.py` \u2192 [__init__.py](../src/utils/__init__.py) \u00b7 [preview](code/src/utils/__init__.md)\n    - `src/utils/config.py` \u2192 [config.py](../src/utils/config.py) \u00b7 [preview](code/src/utils/config.md)\n    - `src/utils/logging.py` \u2192 [logging.py](../src/utils/logging.py) \u00b7 [preview](code/src/utils/logging.md)\n    - `src/utils/README.md` \u2192 [README.md](../src/utils/README.md)\n  - `src/__init__.py` \u2192 [__init__.py](../src/__init__.py) \u00b7 [preview](code/src/__init__.md)\n  - `src/back-arrow.png` \u2192 [back-arrow.png](../src/back-arrow.png)\n  - `src/README.md` \u2192 [README.md](../src/README.md)\n- **[tests/](../tests/README.md)**\n  - `tests/back-arrow.png` \u2192 [back-arrow.png](../tests/back-arrow.png)\n  - `tests/README.md` \u2192 [README.md](../tests/README.md)\n  - `tests/test_optimizers.py` \u2192 [test_optimizers.py](../tests/test_optimizers.py) \u00b7 [preview](code/tests/test_optimizers.md)\n- `README.md` \u2192 [README.md](../README.md)"}, {"path": "docs/README.md", "size": 94, "textual": true, "content": "> Navigation: [Home](index.md) \u00b7 [Site Map](site-map.md) \u00b7 [Code Browser](code-browser.md)\n\n"}, {"path": "docs/link-report.md", "size": 5772, "textual": true, "content": "# Link Report\n\nREADME.md: [Architecture](docs/architecture.md) -> docs/architecture.md [file]\nREADME.md: [Code Browser](docs/code-browser.md) -> docs/code-browser.md [file]\nREADME.md: [Directory](docs/directory-map.md) -> docs/directory-map.md [file]\nREADME.md: [Home](docs/index.md) -> docs/index.md [file]\nREADME.md: [Index](docs/index.md) -> docs/index.md [file]\nREADME.md: [README](configs/README.md) -> configs/README.md [file]\nREADME.md: [README](experiments/README.md) -> experiments/README.md [file]\nREADME.md: [README](scripts/README.md) -> scripts/README.md [file]\nREADME.md: [README](src/README.md) -> src/README.md [file]\nREADME.md: [Site Map](docs/site-map.md) -> docs/site-map.md [file]\nconfigs/README.md: [Code Browser](../docs/code-browser.md) -> ../docs/code-browser.md [file]\nconfigs/README.md: [Home](../docs/index.md) -> ../docs/index.md [file]\nconfigs/README.md: [Site Map](../docs/site-map.md) -> ../docs/site-map.md [file]\ndocs/architecture.md: [Code Browser](code-browser.md) -> code-browser.md [file]\ndocs/architecture.md: [Directory Map](directory-map.md) -> directory-map.md [file]\ndocs/architecture.md: [Home](index.md) -> index.md [file]\ndocs/architecture.md: [Site Map](site-map.md) -> site-map.md [file]\ndocs/directory-map.md: [Code Browser](code-browser.md) -> code-browser.md [file]\ndocs/directory-map.md: [Home](index.md) -> index.md [file]\ndocs/directory-map.md: [Site Map](site-map.md) -> site-map.md [file]\ndocs/directory-map.md: [algorithms](../src/algorithms/README.md) -> ../src/algorithms/README.md [file]\ndocs/directory-map.md: [configs](../configs/README.md) -> ../configs/README.md [file]\ndocs/directory-map.md: [data](../src/data/README.md) -> ../src/data/README.md [file]\ndocs/directory-map.md: [experiments](../experiments/README.md) -> ../experiments/README.md [file]\ndocs/directory-map.md: [models](../src/models/README.md) -> ../src/models/README.md [file]\ndocs/directory-map.md: [scripts](../scripts/README.md) -> ../scripts/README.md [file]\ndocs/directory-map.md: [src](../src/README.md) -> ../src/README.md [file]\ndocs/directory-map.md: [tests](../tests/README.md) -> ../tests/README.md [file]\ndocs/directory-map.md: [training](../src/training/README.md) -> ../src/training/README.md [file]\ndocs/directory-map.md: [utils](../src/utils/README.md) -> ../src/utils/README.md [file]\ndocs/index.md: [Algorithms](../src/algorithms/README.md) -> ../src/algorithms/README.md [file]\ndocs/index.md: [Architecture](architecture.md) -> architecture.md [file]\ndocs/index.md: [Code Browser](code-browser.md) -> code-browser.md [file]\ndocs/index.md: [Configs](../configs/README.md) -> ../configs/README.md [file]\ndocs/index.md: [Data](../src/data/README.md) -> ../src/data/README.md [file]\ndocs/index.md: [Directory Map](directory-map.md) -> directory-map.md [file]\ndocs/index.md: [Home](index.md) -> index.md [file]\ndocs/index.md: [Models](../src/models/README.md) -> ../src/models/README.md [file]\ndocs/index.md: [Site Map](site-map.md) -> site-map.md [file]\ndocs/index.md: [Training Pipeline](../src/training/README.md) -> ../src/training/README.md [file]\nexperiments/2025-10-20-baseline/notes.md: [Code Browser](../../docs/code-browser.md) -> ../../docs/code-browser.md [file]\nexperiments/2025-10-20-baseline/notes.md: [Home](../../docs/index.md) -> ../../docs/index.md [file]\nexperiments/2025-10-20-baseline/notes.md: [Site Map](../../docs/site-map.md) -> ../../docs/site-map.md [file]\nexperiments/README.md: [Code Browser](../docs/code-browser.md) -> ../docs/code-browser.md [file]\nexperiments/README.md: [Home](../docs/index.md) -> ../docs/index.md [file]\nexperiments/README.md: [Site Map](../docs/site-map.md) -> ../docs/site-map.md [file]\nscripts/README.md: [Code Browser](../docs/code-browser.md) -> ../docs/code-browser.md [file]\nscripts/README.md: [Home](../docs/index.md) -> ../docs/index.md [file]\nscripts/README.md: [Site Map](../docs/site-map.md) -> ../docs/site-map.md [file]\nsrc/README.md: [Code Browser](../docs/code-browser.md) -> ../docs/code-browser.md [file]\nsrc/README.md: [Home](../docs/index.md) -> ../docs/index.md [file]\nsrc/README.md: [Site Map](../docs/site-map.md) -> ../docs/site-map.md [file]\nsrc/algorithms/README.md: [Code Browser](../../docs/code-browser.md) -> ../../docs/code-browser.md [file]\nsrc/algorithms/README.md: [Home](../../docs/index.md) -> ../../docs/index.md [file]\nsrc/algorithms/README.md: [Site Map](../../docs/site-map.md) -> ../../docs/site-map.md [file]\nsrc/data/README.md: [Code Browser](../../docs/code-browser.md) -> ../../docs/code-browser.md [file]\nsrc/data/README.md: [Home](../../docs/index.md) -> ../../docs/index.md [file]\nsrc/data/README.md: [Site Map](../../docs/site-map.md) -> ../../docs/site-map.md [file]\nsrc/models/README.md: [Code Browser](../../docs/code-browser.md) -> ../../docs/code-browser.md [file]\nsrc/models/README.md: [Home](../../docs/index.md) -> ../../docs/index.md [file]\nsrc/models/README.md: [Site Map](../../docs/site-map.md) -> ../../docs/site-map.md [file]\nsrc/training/README.md: [Code Browser](../../docs/code-browser.md) -> ../../docs/code-browser.md [file]\nsrc/training/README.md: [Home](../../docs/index.md) -> ../../docs/index.md [file]\nsrc/training/README.md: [Site Map](../../docs/site-map.md) -> ../../docs/site-map.md [file]\nsrc/utils/README.md: [Code Browser](../../docs/code-browser.md) -> ../../docs/code-browser.md [file]\nsrc/utils/README.md: [Home](../../docs/index.md) -> ../../docs/index.md [file]\nsrc/utils/README.md: [Site Map](../../docs/site-map.md) -> ../../docs/site-map.md [file]\ntests/README.md: [Code Browser](../docs/code-browser.md) -> ../docs/code-browser.md [file]\ntests/README.md: [Home](../docs/index.md) -> ../docs/index.md [file]\ntests/README.md: [Site Map](../docs/site-map.md) -> ../docs/site-map.md [file]\n"}, {"path": "docs/code/README.md", "size": 103, "textual": true, "content": "> Navigation: [Home](../index.md) \u00b7 [Site Map](../site-map.md) \u00b7 [Code Browser](../code-browser.md)\n\n"}, {"path": "docs/code/scripts/run_training.md", "size": 1307, "textual": true, "content": "# scripts/run_training.py\n\n[Home](../../index.md) \u00b7 [Site Map](../../site-map.md) \u00b7 [Code Browser](../../code-browser.md) \u00b7 [Folder README](../../../scripts/README.md)\n\n**Open original file:** [run_training.py](../../../scripts/run_training.py)\n\n## Preview\n\n```python\n#!/usr/bin/env python\nimport argparse, os, sys, importlib, random\nfrom pathlib import Path\nimport numpy as np\n\nfrom src.utils.config import load_config\nfrom src.utils.logging import get_logger\nfrom src.training.train import Trainer\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, default=\"configs/default.yaml\", help=\"Path to YAML config.\")\n    args = parser.parse_args()\n\n    cfg = load_config(args.config)\n    exp = cfg.get(\"experiment\", {})\n    out_dir = Path(exp.get(\"output_dir\", \"experiments/run\"))\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    logger = get_logger(name=\"runner\", level=cfg.get(\"logging\", {}).get(\"level\", \"INFO\"), log_dir=cfg.get(\"logging\", {}).get(\"log_dir\", None))\n\n    seed = int(exp.get(\"seed\", 42))\n    set_seed(seed)\n    logger.info(f\"Seed set to {seed}\")\n\n    trainer = Trainer(cfg, logger=logger, out_dir=out_dir)\n    trainer.run()\n\nif __name__ == \"__main__\":\n    main()\n\n```\n"}, {"path": "docs/code/scripts/README.md", "size": 112, "textual": true, "content": "> Navigation: [Home](../../index.md) \u00b7 [Site Map](../../site-map.md) \u00b7 [Code Browser](../../code-browser.md)\n\n"}, {"path": "docs/code/src/__init__.md", "size": 252, "textual": true, "content": "# src/__init__.py\n\n[Home](../../index.md) \u00b7 [Site Map](../../site-map.md) \u00b7 [Code Browser](../../code-browser.md) \u00b7 [Folder README](../../../src/README.md)\n\n**Open original file:** [__init__.py](../../../src/__init__.py)\n\n## Preview\n\n```python\n\n```\n"}, {"path": "docs/code/src/README.md", "size": 112, "textual": true, "content": "> Navigation: [Home](../../index.md) \u00b7 [Site Map](../../site-map.md) \u00b7 [Code Browser](../../code-browser.md)\n\n"}, {"path": "docs/code/src/algorithms/sgd.md", "size": 664, "textual": true, "content": "# src/algorithms/sgd.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/algorithms/README.md)\n\n**Open original file:** [sgd.py](../../../../src/algorithms/sgd.py)\n\n## Preview\n\n```python\nfrom typing import Dict, Any, Tuple\nimport numpy as np\n\nclass SGD:\n    def __init__(self, lr: float = 0.01):\n        self.lr = lr\n        self.state: Dict[str, Any] = {}\n\n    def step(self, params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        for k in params:\n            params[k] = params[k] - self.lr * grads[k]\n        return params\n\n```\n"}, {"path": "docs/code/src/algorithms/__init__.md", "size": 420, "textual": true, "content": "# src/algorithms/__init__.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/algorithms/README.md)\n\n**Open original file:** [__init__.py](../../../../src/algorithms/__init__.py)\n\n## Preview\n\n```python\nfrom .sgd import SGD\nfrom .momentum import SGDMomentum\nfrom .adam import Adam\n\n__all__ = [\"SGD\", \"SGDMomentum\", \"Adam\"]\n\n```\n"}, {"path": "docs/code/src/algorithms/adam.md", "size": 1305, "textual": true, "content": "# src/algorithms/adam.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/algorithms/README.md)\n\n**Open original file:** [adam.py](../../../../src/algorithms/adam.py)\n\n## Preview\n\n```python\nfrom typing import Dict, Any\nimport numpy as np\n\nclass Adam:\n    def __init__(self, lr: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n        self.m: Dict[str, np.ndarray] = {}\n        self.v: Dict[str, np.ndarray] = {}\n        self.t = 0\n\n    def step(self, params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        self.t += 1\n        for k in params:\n            g = grads[k]\n            m = self.m.get(k, np.zeros_like(g))\n            v = self.v.get(k, np.zeros_like(g))\n\n            m = self.beta1 * m + (1 - self.beta1) * g\n            v = self.beta2 * v + (1 - self.beta2) * (g * g)\n\n            m_hat = m / (1 - self.beta1 ** self.t)\n            v_hat = v / (1 - self.beta2 ** self.t)\n\n            params[k] = params[k] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n\n            self.m[k] = m\n            self.v[k] = v\n        return params\n\n```\n"}, {"path": "docs/code/src/algorithms/momentum.md", "size": 880, "textual": true, "content": "# src/algorithms/momentum.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/algorithms/README.md)\n\n**Open original file:** [momentum.py](../../../../src/algorithms/momentum.py)\n\n## Preview\n\n```python\nfrom typing import Dict, Any\nimport numpy as np\n\nclass SGDMomentum:\n    def __init__(self, lr: float = 0.01, momentum: float = 0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.velocity: Dict[str, np.ndarray] = {}\n\n    def step(self, params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        for k in params:\n            v = self.velocity.get(k, np.zeros_like(params[k]))\n            v = self.momentum * v - self.lr * grads[k]\n            self.velocity[k] = v\n            params[k] = params[k] + v\n        return params\n\n```\n"}, {"path": "docs/code/src/algorithms/README.md", "size": 121, "textual": true, "content": "> Navigation: [Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md)\n\n"}, {"path": "docs/code/src/data/loaders.md", "size": 772, "textual": true, "content": "# src/data/loaders.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/data/README.md)\n\n**Open original file:** [loaders.py](../../../../src/data/loaders.py)\n\n## Preview\n\n```python\nfrom typing import Tuple\nimport numpy as np\n\ndef make_synthetic_linear(n_samples: int = 512, n_features: int = 3, noise_std: float = 0.1, seed: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n    rng = np.random.default_rng(seed)\n    X = rng.normal(size=(n_samples, n_features))\n    w_true = rng.normal(size=(n_features, 1))\n    b_true = float(rng.normal())\n    y = X @ w_true + b_true + rng.normal(scale=noise_std, size=(n_samples, 1))\n    return X, y, w_true.flatten(), b_true\n\n```\n"}, {"path": "docs/code/src/data/__init__.md", "size": 282, "textual": true, "content": "# src/data/__init__.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/data/README.md)\n\n**Open original file:** [__init__.py](../../../../src/data/__init__.py)\n\n## Preview\n\n```python\n\n```\n"}, {"path": "docs/code/src/data/README.md", "size": 121, "textual": true, "content": "> Navigation: [Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md)\n\n"}, {"path": "docs/code/src/models/__init__.md", "size": 288, "textual": true, "content": "# src/models/__init__.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/models/README.md)\n\n**Open original file:** [__init__.py](../../../../src/models/__init__.py)\n\n## Preview\n\n```python\n\n```\n"}, {"path": "docs/code/src/models/linear_regression.md", "size": 1042, "textual": true, "content": "# src/models/linear_regression.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/models/README.md)\n\n**Open original file:** [linear_regression.py](../../../../src/models/linear_regression.py)\n\n## Preview\n\n```python\nfrom typing import Dict\nimport numpy as np\n\nclass LinearRegressionModel:\n    def __init__(self, n_features: int):\n        self.params = {\n            \"w\": np.zeros((n_features, 1)),\n            \"b\": np.zeros((1, 1)),\n        }\n\n    def forward(self, X: np.ndarray) -> np.ndarray:\n        return X @ self.params[\"w\"] + self.params[\"b\"]\n\n    def loss_and_grads(self, X: np.ndarray, y: np.ndarray) -> (float, Dict[str, np.ndarray]):\n        # Mean squared error\n        y_pred = self.forward(X)\n        err = y_pred - y\n        loss = float((err ** 2).mean())\n        n = X.shape[0]\n        grads = {\n            \"w\": (2.0 / n) * X.T @ err,\n            \"b\": (2.0 / n) * err.sum(keepdims=True)\n        }\n        return loss, grads\n\n```\n"}, {"path": "docs/code/src/models/README.md", "size": 121, "textual": true, "content": "> Navigation: [Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md)\n\n"}, {"path": "docs/code/src/training/__init__.md", "size": 294, "textual": true, "content": "# src/training/__init__.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/training/README.md)\n\n**Open original file:** [__init__.py](../../../../src/training/__init__.py)\n\n## Preview\n\n```python\n\n```\n"}, {"path": "docs/code/src/training/train.md", "size": 3431, "textual": true, "content": "# src/training/train.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/training/README.md)\n\n**Open original file:** [train.py](../../../../src/training/train.py)\n\n## Preview\n\n```python\nfrom pathlib import Path\nfrom typing import Dict, Any\nimport numpy as np\n\nfrom src.data.loaders import make_synthetic_linear\nfrom src.models.linear_regression import LinearRegressionModel\nfrom src.algorithms import SGD, SGDMomentum, Adam\n\nclass Trainer:\n    def __init__(self, cfg: Dict[str, Any], logger, out_dir: Path):\n        self.cfg = cfg\n        self.logger = logger\n        self.out_dir = out_dir\n\n        data_cfg = cfg.get(\"data\", {})\n        exp_cfg = cfg.get(\"experiment\", {})\n        seed = int(exp_cfg.get(\"seed\", 0))\n\n        X, y, w_true, b_true = make_synthetic_linear(\n            n_samples=int(data_cfg.get(\"n_samples\", 512)),\n            n_features=int(data_cfg.get(\"n_features\", 3)),\n            noise_std=float(data_cfg.get(\"noise_std\", 0.1)),\n            seed=seed,\n        )\n        self.X, self.y = X, y\n        self.w_true, self.b_true = w_true, b_true\n\n        model_cfg = cfg.get(\"model\", {})\n        if model_cfg.get(\"type\", \"linear_regression\") != \"linear_regression\":\n            raise ValueError(\"Only 'linear_regression' model is implemented in this scaffold.\")\n        self.model = LinearRegressionModel(n_features=int(data_cfg.get(\"n_features\", 3)))\n\n        tr = cfg.get(\"training\", {})\n        algo = tr.get(\"algorithm\", \"sgd\").lower()\n        lr = float(tr.get(\"lr\", 0.01))\n\n        if algo == \"sgd\":\n            self.opt = SGD(lr=lr)\n        elif algo == \"momentum\":\n            self.opt = SGDMomentum(lr=lr, momentum=float(tr.get(\"momentum\", 0.9)))\n        elif algo == \"adam\":\n            self.opt = Adam(lr=lr, beta1=float(tr.get(\"beta1\", 0.9)), beta2=float(tr.get(\"beta2\", 0.999)), eps=float(tr.get(\"eps\", 1e-8)))\n        else:\n            raise ValueError(f\"Unknown training.algorithm: {algo}\")\n\n        self.epochs = int(tr.get(\"epochs\", 100))\n        self.batch_size = int(tr.get(\"batch_size\", 64))\n\n    def batches(self, X, y, batch_size):\n        n = X.shape[0]\n        idx = np.arange(n)\n        np.random.shuffle(idx)\n        for i in range(0, n, batch_size):\n            j = idx[i:i+batch_size]\n            yield X[j], y[j]\n\n    def run(self):\n        self.logger.info(\"Starting training\")\n        history = []\n        for epoch in range(1, self.epochs + 1):\n            for Xb, yb in self.batches(self.X, self.y, self.batch_size):\n                loss, grads = self.model.loss_and_grads(Xb, yb)\n                self.model.params = self.opt.step(self.model.params, grads)\n\n            # full pass for logging\n            loss_full, _ = self.model.loss_and_grads(self.X, self.y)\n            history.append({\"epoch\": epoch, \"loss\": loss_full})\n            if epoch % max(1, self.epochs // 10) == 0 or epoch == 1:\n                self.logger.info(f\"epoch {epoch:3d} | loss={loss_full:.6f}\")\n\n        # Save run artifacts\n        np.savez(self.out_dir / \"artifacts.npz\", w=self.model.params[\"w\"], b=self.model.params[\"b\"], w_true=self.w_true, b_true=self.b_true)\n        with open(self.out_dir / \"history.json\", \"w\", encoding=\"utf-8\") as f:\n            import json\n            json.dump(history, f, indent=2)\n        self.logger.info(\"Training complete. Artifacts written to %s\", str(self.out_dir))\n\n```\n"}, {"path": "docs/code/src/training/README.md", "size": 121, "textual": true, "content": "> Navigation: [Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md)\n\n"}, {"path": "docs/code/src/training/legacy/grad_desc_prob.md", "size": 17805, "textual": true, "content": "# src/training/legacy/grad_desc_prob.py\n\n[Home](../../../../index.md) \u00b7 [Site Map](../../../../site-map.md) \u00b7 [Code Browser](../../../../code-browser.md)\n\n**Open original file:** [grad_desc_prob.py](../../../../../src/training/legacy/grad_desc_prob.py)\n\n## Preview\n\n```python\n# \"Soft Safe\" Gradient Descent Playground\n# ------------------------------------------------------------\n# This single Python cell gives you:\n#   \u2022 A smooth nonconvex loss on a 3\u2011torus (three circular dials).\n#   \u2022 Multiple optimizers: fixed-step GD, backtracking GD, momentum/Nesterov, Adam.\n#   \u2022 Options for learning\u2011rate schedules, noise, multi\u2011start, and diagnostics.\n#   \u2022 Heavy comments with the math, when to use which option, and why.\n#\n# RULES (for this environment):\n#   - Uses Matplotlib (no seaborn), one chart per figure, no custom colors/styles.\n#\n# You can scroll through the code comments as a mini\u2011tutorial.\n# At the bottom, we run a few demo experiments and draw simple plots.\n#\n# Notation:\n#   \u03b8 \u2208 T^3 where T is a circle; each \u03b8_i is wrapped to [-\u03c0, \u03c0).\n#   d(\u03b8, \u03b8*) = wrap(\u03b8 - \u03b8*) is the shortest signed angular difference.\n#\n# GLOBAL OBJECTS created:\n#   - theta_star: the hidden safe combination (randomized per run).\n#   - couplings: dict with c1,c2,c3 shaping nonconvexity.\n#\n# ------------------------------------------------------------\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(7)  # change seed to reshuffle the secret combo each run\n\n# --------------------------\n# Geometry utilities on the torus T^3\n# --------------------------\ndef wrap_angle(a):\n    \"\"\"\n    Wrap a (possibly vector) angle to [-\u03c0, \u03c0).\n    This defines the equivalence relation of angles modulo 2\u03c0 and\n    is essential for optimization on a torus rather than R^3.\n    \"\"\"\n    return (a + np.pi) % (2*np.pi) - np.pi\n\ndef angdiff(a, b):\n    \"\"\"\n    Shortest signed angular difference in [-\u03c0, \u03c0):\n    angdiff(a,b) = wrap(a - b).\n    \"\"\"\n    return wrap_angle(a - b)\n\ndef torus_distance(a, b):\n    \"\"\"\n    L2 distance on the torus: ||wrap(a-b)||_2.\n    This respects periodicity and is the relevant notion of \"closeness.\"\n    \"\"\"\n    d = angdiff(np.asarray(a), np.asarray(b))\n    return float(np.linalg.norm(d))\n\n# --------------------------\n# Loss: \"Soft Safe\" with gentle couplings\n# --------------------------\n# Global minimum is at \u03b8 = \u03b8* (mod 2\u03c0). The base term is convex on each angle,\n# but couplings introduce soft nonconvex structure and benign local minima.\ntheta_star = rng.uniform(-np.pi, np.pi, size=3)  # secret target\ncouplings = dict(c1=0.30, c2=0.20, c3=0.10)\n\ndef lock_loss(theta, theta_star=theta_star, couplings=couplings):\n    r\"\"\"\n    L(\u03b8) = \u03a3_i (1 - cos d_i)\n           + c1 * (1 - cos(d0 + 2 d1))\n           + c2 * (1 - cos(2 d1 - d2))\n           + c3 * (1 - cos(3 d0 - d1 + d2))\n    where d = wrap(\u03b8 - \u03b8*).\n\n    WHY THIS FORM:\n    - 1 - cos(d_i) has a unique minimum at d_i = 0, quadratic near 0:\n        1 - cos(d_i) \u2248 0.5 d_i^2.\n      Its gradient is sin(d_i). This creates a bowl around \u03b8*.\n    - Coupling terms are low\u2011weight perturbations that keep the same global\n      minimizer but create gentle local basins so GD has to think.\n    \"\"\"\n    theta = np.asarray(theta, dtype=float)\n    d = angdiff(theta, theta_star)\n    base = float(np.sum(1 - np.cos(d)))\n    c1, c2, c3 = couplings[\"c1\"], couplings[\"c2\"], couplings[\"c3\"]\n    t1 = 1 - np.cos(d[0] + 2*d[1])\n    t2 = 1 - np.cos(2*d[1] - d[2])\n    t3 = 1 - np.cos(3*d[0] - d[1] + d[2])\n    return base + c1*t1 + c2*t2 + c3*t3\n\ndef lock_grad(theta, theta_star=theta_star, couplings=couplings):\n    r\"\"\"\n    Analytic gradient \u2207L(\u03b8). Using d = wrap(\u03b8 - \u03b8*).\n    For a term 1 - cos(u), d/du = sin(u).\n    Chain rule gives \u2202/\u2202\u03b8_i [1 - cos(d_i)] = sin(d_i) * \u2202d_i/\u2202\u03b8_i.\n    Ignoring the measure\u2011zero wrap kinks, \u2202d_i/\u2202\u03b8_i = 1 almost everywhere.\n\n    Coupling terms:\n      t1 = 1 - cos(d0 + 2 d1)     \u21d2 \u2207 = sin(d0 + 2 d1) * [1, 2, 0]\n      t2 = 1 - cos(2 d1 - d2)     \u21d2 \u2207 = sin(2 d1 - d2) * [0, 2, -1]\n      t3 = 1 - cos(3 d0 - d1 + d2)\u21d2 \u2207 = sin(3 d0 - d1 + d2) * [3, -1, 1]\n\n    NOTE: On the torus, take a gradient step in R^3, then wrap.\n    \"\"\"\n    theta = np.asarray(theta, dtype=float)\n    d = angdiff(theta, theta_star)\n    g = np.zeros(3, dtype=float)\n\n    # base\n    g += np.sin(d)\n\n    c1, c2, c3 = couplings[\"c1\"], couplings[\"c2\"], couplings[\"c3\"]\n\n    s1 = np.sin(d[0] + 2*d[1])\n    g[0] += c1 * s1 * 1.0\n    g[1] += c1 * s1 * 2.0\n\n    s2 = np.sin(2*d[1] - d[2])\n    g[1] += c2 * s2 * 2.0\n    g[2] += c2 * s2 * (-1.0)\n\n    s3 = np.sin(3*d[0] - d[1] + d[2])\n    g[0] += c3 * s3 * 3.0\n    g[1] += c3 * s3 * (-1.0)\n    g[2] += c3 * s3 * 1.0\n\n    return g\n\n# --------------------------\n# Gradient checker (optional but educational)\n# --------------------------\ndef check_grad(f, grad, x, h=1e-5):\n    \"\"\"\n    Central finite\u2011difference gradient check on the torus.\n    Returns relative error ||g_fd - g|| / max(1, ||g||, ||g_fd||).\n\n    Use this once to sanity\u2011check your analytic gradient.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    g_fd = np.zeros_like(x)\n    for i in range(len(x)):\n        e = np.zeros_like(x); e[i] = 1.0\n        xph = wrap_angle(x + h*e)\n        xmh = wrap_angle(x - h*e)\n        g_fd[i] = (f(xph) - f(xmh)) / (2*h)\n    g = grad(x)\n    denom = max(1.0, float(np.linalg.norm(g)), float(np.linalg.norm(g_fd)))\n    rel = float(np.linalg.norm(g_fd - g)) / denom\n    return rel, g, g_fd\n\n# --------------------------\n# Learning\u2011rate schedules\n# --------------------------\ndef lr_constant(lr0):\n    \"\"\"Constant LR: \u03b7_k = lr0. Good when a line search is expensive and the landscape is tame.\"\"\"\n    def s(k): return lr0\n    return s\n\ndef lr_geometric(lr0, gamma=0.99):\n    \"\"\"Geometric decay: \u03b7_k = lr0 * \u03b3^k. Useful when you want coarse steps early, fine steps late.\"\"\"\n    def s(k): return lr0 * (gamma ** k)\n    return s\n\ndef lr_inverse_sqrt(lr0, warmup=10):\n    \"\"\"Inverse\u2011sqrt: \u03b7_k = lr0 / sqrt(k + warmup). A classic schedule with diminishing steps.\"\"\"\n    def s(k): return lr0 / math.sqrt(k + warmup)\n    return s\n\n# --------------------------\n# Core optimizers on the torus\n# --------------------------\ndef gd_fixed(f, grad, x0, lr_schedule, max_iter=2000, tol_g=1e-9, tol_x=1e-10,\n             noise_std=0.0, noise_every=None, callback=None):\n    r\"\"\"\n    Fixed\u2011step gradient descent on T^3.\n\n    Update:\n        g_k = \u2207f(x_k)\n        \u03b7_k = schedule(k)\n        x_{k+1} = wrap(x_k - \u03b7_k g_k)\n\n    Stopping:\n        ||g_k|| < tol_g   or   ||wrap(x_{k+1}-x_k)|| < tol_x   or   k hits max_iter.\n\n    Options:\n      - lr_schedule: one of the schedule factories above.\n      - noise_std / noise_every: add small Gaussian noise to escape shallow traps.\n        x_{k+1} \u2190 wrap(x_{k+1} + N(0, noise_std^2 I)) every 'noise_every' steps.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    xs = [x.copy()]; fs = [float(f(x))]\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = float(np.linalg.norm(g))\n        if gnorm < tol_g: break\n        step = lr_schedule(k)\n        xn = wrap_angle(x - step * g)\n        if noise_every is not None and ((k+1) % noise_every == 0) and noise_std > 0:\n            xn = wrap_angle(xn + rng.normal(0, noise_std, size=xn.shape))\n        xs.append(xn.copy()); fs.append(float(f(xn)))\n        if float(np.linalg.norm(angdiff(xn, x))) < tol_x: x = xn; break\n        x = xn\n    return x, np.array(xs), np.array(fs)\n\ndef gd_backtracking(f, grad, x0, lr_init=0.3, alpha=1e-4, beta=0.6, max_iter=2000,\n                    tol_g=1e-9, tol_x=1e-10, callback=None):\n    r\"\"\"\n    Backtracking line search (Armijo). Robust first choice if you're allergic to tuning.\n\n    Pick step \u03b7 starting from lr_init, shrink by \u03b2 \u2208 (0,1) until\n        f(x - \u03b7 g) \u2264 f(x) - \u03b1 \u03b7 ||g||^2\n    where \u03b1 \u2208 (0, 0.5) is the sufficient\u2011decrease parameter.\n\n    WHY:\n      - Automatic step control; works well when curvature varies across the space.\n      - More function evals per iteration, but fewer faceplants.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    xs = [x.copy()]; fx = float(f(x)); fs = [fx]\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = float(np.linalg.norm(g))\n        if gnorm < tol_g: break\n        step = lr_init\n        # Candidate\n        xn = wrap_angle(x - step * g)\n        fn = float(f(xn))\n        # Armijo backtracking\n        iters = 0\n        while fn > fx - alpha * step * (gnorm**2):\n            step *= beta\n            xn = wrap_angle(x - step * g)\n            fn = float(f(xn))\n            iters += 1\n            if iters > 60:\n                break\n        xs.append(xn.copy()); fs.append(fn)\n        if float(np.linalg.norm(angdiff(xn, x))) < tol_x:\n            x = xn; fx = fn; break\n        x = xn; fx = fn\n    return x, np.array(xs), np.array(fs)\n\ndef gd_momentum(f, grad, x0, lr=0.2, mu=0.9, nesterov=False, max_iter=2000,\n                tol_g=1e-9, tol_x=1e-10, callback=None):\n    r\"\"\"\n    Momentum / Nesterov on T^3.\n\n    Classical momentum:\n        v_{k+1} = \u03bc v_k - \u03b7 \u2207f(x_k)\n        x_{k+1} = wrap(x_k + v_{k+1})\n\n    Nesterov (look\u2011ahead) gradient:\n        g_k = \u2207f(x_k + \u03bc v_k)\n        v_{k+1} = \u03bc v_k - \u03b7 g_k\n        x_{k+1} = wrap(x_k + v_{k+1})\n\n    WHEN:\n      - Use when the landscape has long, gently sloped valleys; momentum accelerates.\n      - Nesterov can reduce overshoot by peeking ahead.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    v = np.zeros_like(x)\n    xs = [x.copy()]; fs = [float(f(x))]\n    for k in range(max_iter):\n        if nesterov:\n            g = grad(wrap_angle(x + mu * v))\n        else:\n            g = grad(x)\n        gnorm = float(np.linalg.norm(g))\n        if gnorm < tol_g: break\n        v = mu * v - lr * g\n        xn = wrap_angle(x + v)\n        xs.append(xn.copy()); fs.append(float(f(xn)))\n        if float(np.linalg.norm(angdiff(xn, x))) < tol_x:\n            x = xn; break\n        x = xn\n    return x, np.array(xs), np.array(fs)\n\ndef adam_torus(f, grad, x0, lr=0.1, beta1=0.9, beta2=0.999, eps=1e-8,\n               max_iter=4000, tol_g=1e-9, tol_x=1e-10, callback=None):\n    r\"\"\"\n    Adam optimizer on the torus.\n\n    m_{k+1} = \u03b21 m_k + (1-\u03b21) g_k\n    v_{k+1} = \u03b22 v_k + (1-\u03b22) g_k^2\n    m\u0302 = m_{k+1} / (1-\u03b21^{k+1})\n    v\u0302 = v_{k+1} / (1-\u03b22^{k+1})\n    x_{k+1} = wrap(x_k - lr * m\u0302 / (sqrt(v\u0302) + eps))\n\n    WHEN:\n      - Coordinates have different curvature/scales. Adam adaptively rescales steps.\n      - Often converges with minimal tuning; beware too\u2011large lr.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    xs = [x.copy()]; fs = [float(f(x))]\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = float(np.linalg.norm(g))\n        if gnorm < tol_g: break\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g * g)\n        mhat = m / (1 - beta1 ** (k + 1))\n        vhat = v / (1 - beta2 ** (k + 1))\n        step = lr * mhat / (np.sqrt(vhat) + eps)\n        xn = wrap_angle(x - step)\n        xs.append(xn.copy()); fs.append(float(f(xn)))\n        if float(np.linalg.norm(angdiff(xn, x))) < tol_x:\n            x = xn; break\n        x = xn\n    return x, np.array(xs), np.array(fs)\n\n# --------------------------\n# Multi\u2011start driver and success metrics\n# --------------------------\ndef solve_multistart(optimizer_fn, f, grad, n_starts=40, **opt_kwargs):\n    \"\"\"\n    Try many random initializations; keep best result and compute success rate.\n    A run is 'successful' if it recovers \u03b8* within tight torus distance and loss.\n    \"\"\"\n    best = None\n    successes = 0\n    paths, losses = None, None\n    for _ in range(n_starts):\n        x0 = rng.uniform(-np.pi, np.pi, size=3)\n        x, xs, fs = optimizer_fn(f, grad, x0, **opt_kwargs)\n        val = f(x)\n        dist = torus_distance(x, theta_star)\n        ok = (dist < 1e-3) and (val < 1e-8)\n        if ok: successes += 1\n        if (best is None) or (val < best[1]):\n            best = (x, val, x0)\n            paths, losses = xs, fs\n    success_rate = successes / n_starts\n    return dict(best_x=best[0], best_val=best[1], best_start=best[2],\n                best_path=paths, best_losses=losses, success_rate=success_rate)\n\n# --------------------------\n# 1) Sanity check the gradient\n# --------------------------\nx_check = rng.uniform(-np.pi, np.pi, size=3)\nrel_err, g_analytic, g_fd = check_grad(lock_loss, lock_grad, x_check)\nprint(\"Gradient check at random point\")\nprint(\"  point:\", np.round(x_check, 4))\nprint(\"  rel error:\", f\"{rel_err:.3e}\", \"(<1e-6 is excellent; <1e-4 is fine for this problem)\")\n\n# --------------------------\n# 2) Single\u2011run comparisons from the same start\n# --------------------------\nx0 = rng.uniform(-np.pi, np.pi, size=3)\nprint(\"\\nSingle\u2011run comparisons from a shared start:\")\nprint(\"  secret \u03b8*:\", np.round(theta_star, 6))\nprint(\"  start \u03b80 :\", np.round(x0, 6))\n\n# Fixed\u2011step with geometric decay\nx_fixed, path_fixed, loss_fixed = gd_fixed(\n    lock_loss, lock_grad, x0,\n    lr_schedule=lr_geometric(lr0=0.35, gamma=0.985),\n    max_iter=800\n)\n\n# Backtracking\nx_bt, path_bt, loss_bt = gd_backtracking(\n    lock_loss, lock_grad, x0,\n    lr_init=0.35, alpha=1e-4, beta=0.6, max_iter=400\n)\n\n# Momentum (Nesterov on)\nx_mom, path_mom, loss_mom = gd_momentum(\n    lock_loss, lock_grad, x0,\n    lr=0.22, mu=0.9, nesterov=True, max_iter=800\n)\n\n# Adam\nx_adam, path_adam, loss_adam = adam_torus(\n    lock_loss, lock_grad, x0,\n    lr=0.12, max_iter=1200\n)\n\ndef report(name, x_final, losses):\n    dist = torus_distance(x_final, theta_star)\n    print(f\"  {name:12s} | iters={len(losses)-1:4d}  loss={losses[-1]:.3e}  dist={dist:.3e}\")\n\nreport(\"fixed\u2011step\", x_fixed, loss_fixed)\nreport(\"backtracking\", x_bt, loss_bt)\nreport(\"momentum+NAG\", x_mom, loss_mom)\nreport(\"adam\", x_adam, loss_adam)\n\n# Plot loss curves (one figure per optimizer)\nplt.figure()\nplt.plot(np.arange(len(loss_fixed)), loss_fixed)\nplt.title(\"Fixed\u2011step GD: loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\nplt.figure()\nplt.plot(np.arange(len(loss_bt)), loss_bt)\nplt.title(\"Backtracking GD: loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\nplt.figure()\nplt.plot(np.arange(len(loss_mom)), loss_mom)\nplt.title(\"Momentum (Nesterov): loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\nplt.figure()\nplt.plot(np.arange(len(loss_adam)), loss_adam)\nplt.title(\"Adam: loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\n# --------------------------\n# 3) Success probability vs learning rate (fixed\u2011step)\n# --------------------------\nlrs = [0.05, 0.1, 0.18, 0.25, 0.35, 0.5]\nsuccess = []\nfor lr0 in lrs:\n    res = solve_multistart(\n        gd_fixed, lock_loss, lock_grad, n_starts=30,\n        lr_schedule=lr_constant(lr0), max_iter=800\n    )\n    success.append(res[\"success_rate\"])\n\nplt.figure()\nplt.plot(lrs, success, marker='o')\nplt.title(\"Fixed\u2011step GD: success rate vs learning rate\")\nplt.xlabel(\"learning rate\")\nplt.ylabel(\"success fraction\")\nplt.ylim(0, 1.05)\nplt.show()\n\n# --------------------------\n# 4) Multi\u2011start with backtracking (robust baseline)\n# --------------------------\nres_bt = solve_multistart(\n    gd_backtracking, lock_loss, lock_grad, n_starts=60,\n    lr_init=0.35, alpha=1e-4, beta=0.6, max_iter=600\n)\nx_best = res_bt[\"best_x\"]\ndist_best = torus_distance(x_best, theta_star)\nprint(\"\\nMulti\u2011start (backtracking):\")\nprint(\"  best start:\", np.round(res_bt[\"best_start\"], 6))\nprint(\"  recovered :\", np.round(x_best, 6))\nprint(\"  distance to \u03b8*:\", f\"{dist_best:.3e}\")\nprint(\"  final loss:\", f\"{res_bt['best_val']:.3e}\")\nprint(\"  success rate:\", f\"{res_bt['success_rate']:.2f} over 60 starts\")\n\nplt.figure()\nplt.plot(np.arange(len(res_bt[\"best_losses\"])), res_bt[\"best_losses\"])\nplt.title(\"Best run (backtracking): loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\n# --------------------------\n# 5) Visual intuition: 2D contour slice with a path overlay\n#    Fix \u03b81 and visualize L(\u03b80, \u03b82 | \u03b81=const). Overlay the best backtracking path.\n# --------------------------\ntheta1_fixed = float(res_bt[\"best_path\"][0][1])  # take initial \u03b81 for the best run\ngrid = 150\nt0 = np.linspace(-np.pi, np.pi, grid)\nt2 = np.linspace(-np.pi, np.pi, grid)\nT0, T2 = np.meshgrid(t0, t2, indexing='xy')\nZ = np.zeros_like(T0)\n\n# Build the slice; this is compute heavy but fine at 150x150\nfor i in range(grid):\n    for j in range(grid):\n        th = np.array([T0[i, j], theta1_fixed, T2[i, j]])\n        Z[i, j] = lock_loss(th)\n\nplt.figure()\n# contourf uses a default colormap; we don't specify any styles.\ncs = plt.contourf(T0, T2, Z, levels=25)\nplt.colorbar()\nplt.title(r\"Contour slice: $L(\\theta_0,\\theta_2\\,|\\,\\theta_1=\\mathrm{const})$\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_2$\")\n\n# Overlay the best path (project to this slice by taking its \u03b80, \u03b82)\np = np.array(res_bt[\"best_path\"])\nplt.plot(p[:,0], p[:,2], marker='o', linewidth=1)\nplt.show()\n\n# --------------------------\n# 6) Optional: demonstrate noise annealing escape\n# --------------------------\n# We'll run a fixed\u2011step with tiny periodic noise; sometimes this bumps you out of a shallow trap.\nx0_noise = rng.uniform(-np.pi, np.pi, size=3)\nx_noise, path_noise, loss_noise = gd_fixed(\n    lock_loss, lock_grad, x0_noise,\n    lr_schedule=lr_constant(0.22),\n    max_iter=800,\n    noise_std=0.02, noise_every=10\n)\nprint(\"\\nNoise\u2011annealed fixed\u2011step example:\")\nprint(\"  start:\", np.round(x0_noise, 6))\nprint(\"  end  :\", np.round(x_noise, 6))\nprint(\"  dist :\", f\"{torus_distance(x_noise, theta_star):.3e}\")\nplt.figure()\nplt.plot(np.arange(len(loss_noise)), loss_noise)\nplt.title(\"Fixed\u2011step with periodic noise: loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n```\n"}, {"path": "docs/code/src/training/legacy/README.md", "size": 130, "textual": true, "content": "> Navigation: [Home](../../../../index.md) \u00b7 [Site Map](../../../../site-map.md) \u00b7 [Code Browser](../../../../code-browser.md)\n\n"}, {"path": "docs/code/src/utils/logging.md", "size": 1190, "textual": true, "content": "# src/utils/logging.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/utils/README.md)\n\n**Open original file:** [logging.py](../../../../src/utils/logging.py)\n\n## Preview\n\n```python\nimport logging\nfrom pathlib import Path\nfrom typing import Optional\n\ndef get_logger(name: str = \"app\", level: str = \"INFO\", log_dir: Optional[str] = None) -> logging.Logger:\n    logger = logging.getLogger(name)\n    if logger.handlers:\n        return logger  # avoid duplicate handlers in notebooks\n\n    numeric_level = getattr(logging, level.upper(), logging.INFO)\n    logger.setLevel(numeric_level)\n\n    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\")\n\n    stream = logging.StreamHandler()\n    stream.setFormatter(fmt)\n    stream.setLevel(numeric_level)\n    logger.addHandler(stream)\n\n    if log_dir:\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(Path(log_dir) / f\"{name}.log\")\n        file_handler.setFormatter(fmt)\n        file_handler.setLevel(numeric_level)\n        logger.addHandler(file_handler)\n\n    return logger\n\n```\n"}, {"path": "docs/code/src/utils/config.md", "size": 979, "textual": true, "content": "# src/utils/config.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/utils/README.md)\n\n**Open original file:** [config.py](../../../../src/utils/config.py)\n\n## Preview\n\n```python\nfrom pathlib import Path\nfrom typing import Any, Dict\n\ndef load_config(path: str) -> Dict[str, Any]:\n    \"\"\"Load a YAML config. Falls back with a clear message if PyYAML isn't installed.\"\"\"\n    try:\n        import yaml  # type: ignore\n    except Exception as e:\n        raise RuntimeError(\n            \"PyYAML is required to load configs. Install with `pip install pyyaml`.\"\n        ) from e\n\n    p = Path(path)\n    if not p.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n    with p.open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f)\n    if not isinstance(data, dict):\n        raise ValueError(\"Top-level YAML must be a mapping (dict).\" )\n    return data\n\n```\n"}, {"path": "docs/code/src/utils/__init__.md", "size": 285, "textual": true, "content": "# src/utils/__init__.py\n\n[Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md) \u00b7 [Folder README](../../../../src/utils/README.md)\n\n**Open original file:** [__init__.py](../../../../src/utils/__init__.py)\n\n## Preview\n\n```python\n\n```\n"}, {"path": "docs/code/src/utils/README.md", "size": 121, "textual": true, "content": "> Navigation: [Home](../../../index.md) \u00b7 [Site Map](../../../site-map.md) \u00b7 [Code Browser](../../../code-browser.md)\n\n"}, {"path": "docs/code/tests/test_optimizers.md", "size": 1094, "textual": true, "content": "# tests/test_optimizers.py\n\n[Home](../../index.md) \u00b7 [Site Map](../../site-map.md) \u00b7 [Code Browser](../../code-browser.md) \u00b7 [Folder README](../../../tests/README.md)\n\n**Open original file:** [test_optimizers.py](../../../tests/test_optimizers.py)\n\n## Preview\n\n```python\nimport numpy as np\nfrom src.algorithms import SGD, SGDMomentum, Adam\n\ndef quad_loss_and_grad(x):\n    # f(x) = (x-3)^2\n    f = float((x - 3.0) ** 2)\n    g = 2.0 * (x - 3.0)\n    return f, g\n\ndef run_optimizer(opt, steps=200, lr=0.05):\n    x = 0.0\n    hist = []\n    for _ in range(steps):\n        f, g = quad_loss_and_grad(x)\n        x = opt.step({\"x\": np.array([[x]])}, {\"x\": np.array([[g]])})[\"x\"][0,0]\n        hist.append(f)\n    return hist\n\ndef test_sgd_decreases_loss():\n    opt = SGD(lr=0.05)\n    hist = run_optimizer(opt)\n    assert hist[-1] < hist[0]\n\ndef test_momentum_decreases_loss():\n    opt = SGDMomentum(lr=0.05, momentum=0.9)\n    hist = run_optimizer(opt)\n    assert hist[-1] < hist[0]\n\ndef test_adam_decreases_loss():\n    opt = Adam(lr=0.05)\n    hist = run_optimizer(opt)\n    assert hist[-1] < hist[0]\n\n```\n"}, {"path": "docs/code/tests/README.md", "size": 112, "textual": true, "content": "> Navigation: [Home](../../index.md) \u00b7 [Site Map](../../site-map.md) \u00b7 [Code Browser](../../code-browser.md)\n\n"}, {"path": "experiments/back-arrow.png", "size": 47253, "textual": false, "content": null}, {"path": "experiments/README.md", "size": 718, "textual": true, "content": "> Navigation: [Home](../docs/index.md) \u00b7 [Site Map](../docs/site-map.md) \u00b7 [Code Browser](../docs/code-browser.md)\n\n<style>\n  .back-link {\n    position: fixed;       /* stay in viewport as you scroll */\n    top: 10px;              /* adjust vertical offset */\n    left: 10px;             /* adjust horizontal offset */\n    z-index: 1000;          /* sit above most stuff */\n  }\n\n  .back-link img {\n    width: 30px;             /* size of your arrow \u2014 change as needed */\n    height: auto;\n    cursor: pointer;\n  }\n</style>\n\n<a href=\"../README.md\" class=\"back-link\">\n  <img src=\"back-arrow.png\" alt=\"Back\">\n</a>\n\n\n# Experiments\n\nEach run writes outputs to a timestamped folder. Keep notes, logs, and artifacts here."}, {"path": "experiments/2025-10-20-baseline/.gitkeep", "size": 0, "textual": true, "content": ""}, {"path": "experiments/2025-10-20-baseline/notes.md", "size": 252, "textual": true, "content": "> Navigation: [Home](../../docs/index.md) \u00b7 [Site Map](../../docs/site-map.md) \u00b7 [Code Browser](../../docs/code-browser.md)\n\n# Baseline notes (2025-10-20)\n\n- Purpose: sanity check the pipeline with default YAML.\n- Outcome: fill this in after running."}, {"path": "experiments/2025-10-20-baseline/README.md", "size": 127, "textual": true, "content": "> Navigation: [Home](../../docs/index.md) \u00b7 [Site Map](../../docs/site-map.md) \u00b7 [Code Browser](../../docs/code-browser.md)\n\n"}, {"path": "scripts/back-arrow.png", "size": 47253, "textual": false, "content": null}, {"path": "scripts/README.md", "size": 742, "textual": true, "content": "> Navigation: [Home](../docs/index.md) \u00b7 [Site Map](../docs/site-map.md) \u00b7 [Code Browser](../docs/code-browser.md)\n\n<style>\n  .back-link {\n    position: fixed;       /* stay in viewport as you scroll */\n    top: 10px;              /* adjust vertical offset */\n    left: 10px;             /* adjust horizontal offset */\n    z-index: 1000;          /* sit above most stuff */\n  }\n\n  .back-link img {\n    width: 30px;             /* size of your arrow \u2014 change as needed */\n    height: auto;\n    cursor: pointer;\n  }\n</style>\n\n<a href=\"../README.md\" class=\"back-link\">\n  <img src=\"back-arrow.png\" alt=\"Back\">\n</a>\n\n\n# scripts\n\nEntry points and utilities.\n\n- `run_training.py` main runner that loads a YAML and launches the training pipeline."}, {"path": "scripts/run_training.py", "size": 1031, "textual": true, "content": "#!/usr/bin/env python\nimport argparse, os, sys, importlib, random\nfrom pathlib import Path\nimport numpy as np\n\nfrom src.utils.config import load_config\nfrom src.utils.logging import get_logger\nfrom src.training.train import Trainer\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--config\", type=str, default=\"configs/default.yaml\", help=\"Path to YAML config.\")\n    args = parser.parse_args()\n\n    cfg = load_config(args.config)\n    exp = cfg.get(\"experiment\", {})\n    out_dir = Path(exp.get(\"output_dir\", \"experiments/run\"))\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    logger = get_logger(name=\"runner\", level=cfg.get(\"logging\", {}).get(\"level\", \"INFO\"), log_dir=cfg.get(\"logging\", {}).get(\"log_dir\", None))\n\n    seed = int(exp.get(\"seed\", 42))\n    set_seed(seed)\n    logger.info(f\"Seed set to {seed}\")\n\n    trainer = Trainer(cfg, logger=logger, out_dir=out_dir)\n    trainer.run()\n\nif __name__ == \"__main__\":\n    main()\n"}, {"path": "src/__init__.py", "size": 0, "textual": true, "content": ""}, {"path": "src/back-arrow.png", "size": 47253, "textual": false, "content": null}, {"path": "src/README.md", "size": 686, "textual": true, "content": "> Navigation: [Home](../docs/index.md) \u00b7 [Site Map](../docs/site-map.md) \u00b7 [Code Browser](../docs/code-browser.md)\n\n<style>\n  .back-link {\n    position: fixed;       /* stay in viewport as you scroll */\n    top: 10px;              /* adjust vertical offset */\n    left: 10px;             /* adjust horizontal offset */\n    z-index: 1000;          /* sit above most stuff */\n  }\n\n  .back-link img {\n    width: 30px;             /* size of your arrow \u2014 change as needed */\n    height: auto;\n    cursor: pointer;\n  }\n</style>\n\n<a href=\"../README.md\" class=\"back-link\">\n  <img src=\"back-arrow.png\" alt=\"Back\">\n</a>\n\n\n# src/\n\nCore code. Each subfolder has a tight, single responsibility."}, {"path": "src/algorithms/sgd.py", "size": 379, "textual": true, "content": "from typing import Dict, Any, Tuple\nimport numpy as np\n\nclass SGD:\n    def __init__(self, lr: float = 0.01):\n        self.lr = lr\n        self.state: Dict[str, Any] = {}\n\n    def step(self, params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        for k in params:\n            params[k] = params[k] - self.lr * grads[k]\n        return params\n"}, {"path": "src/algorithms/__init__.py", "size": 120, "textual": true, "content": "from .sgd import SGD\nfrom .momentum import SGDMomentum\nfrom .adam import Adam\n\n__all__ = [\"SGD\", \"SGDMomentum\", \"Adam\"]\n"}, {"path": "src/algorithms/README.md", "size": 241, "textual": true, "content": "> Navigation: [Home](../../docs/index.md) \u00b7 [Site Map](../../docs/site-map.md) \u00b7 [Code Browser](../../docs/code-browser.md)\n\n# algorithms\n\nOptimizers live here. Each implements a `step(params, grads)` method and carries any internal state."}, {"path": "src/algorithms/adam.py", "size": 1017, "textual": true, "content": "from typing import Dict, Any\nimport numpy as np\n\nclass Adam:\n    def __init__(self, lr: float = 0.001, beta1: float = 0.9, beta2: float = 0.999, eps: float = 1e-8):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n        self.m: Dict[str, np.ndarray] = {}\n        self.v: Dict[str, np.ndarray] = {}\n        self.t = 0\n\n    def step(self, params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        self.t += 1\n        for k in params:\n            g = grads[k]\n            m = self.m.get(k, np.zeros_like(g))\n            v = self.v.get(k, np.zeros_like(g))\n\n            m = self.beta1 * m + (1 - self.beta1) * g\n            v = self.beta2 * v + (1 - self.beta2) * (g * g)\n\n            m_hat = m / (1 - self.beta1 ** self.t)\n            v_hat = v / (1 - self.beta2 ** self.t)\n\n            params[k] = params[k] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n\n            self.m[k] = m\n            self.v[k] = v\n        return params\n"}, {"path": "src/algorithms/momentum.py", "size": 580, "textual": true, "content": "from typing import Dict, Any\nimport numpy as np\n\nclass SGDMomentum:\n    def __init__(self, lr: float = 0.01, momentum: float = 0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.velocity: Dict[str, np.ndarray] = {}\n\n    def step(self, params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        for k in params:\n            v = self.velocity.get(k, np.zeros_like(params[k]))\n            v = self.momentum * v - self.lr * grads[k]\n            self.velocity[k] = v\n            params[k] = params[k] + v\n        return params\n"}, {"path": "src/data/loaders.py", "size": 493, "textual": true, "content": "from typing import Tuple\nimport numpy as np\n\ndef make_synthetic_linear(n_samples: int = 512, n_features: int = 3, noise_std: float = 0.1, seed: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n    rng = np.random.default_rng(seed)\n    X = rng.normal(size=(n_samples, n_features))\n    w_true = rng.normal(size=(n_features, 1))\n    b_true = float(rng.normal())\n    y = X @ w_true + b_true + rng.normal(scale=noise_std, size=(n_samples, 1))\n    return X, y, w_true.flatten(), b_true\n"}, {"path": "src/data/__init__.py", "size": 0, "textual": true, "content": ""}, {"path": "src/data/README.md", "size": 170, "textual": true, "content": "> Navigation: [Home](../../docs/index.md) \u00b7 [Site Map](../../docs/site-map.md) \u00b7 [Code Browser](../../docs/code-browser.md)\n\n# data\n\nData loaders and dataset utilities."}, {"path": "src/models/__init__.py", "size": 0, "textual": true, "content": ""}, {"path": "src/models/README.md", "size": 181, "textual": true, "content": "> Navigation: [Home](../../docs/index.md) \u00b7 [Site Map](../../docs/site-map.md) \u00b7 [Code Browser](../../docs/code-browser.md)\n\n# models\n\nModel definitions and loss/grad computation."}, {"path": "src/models/linear_regression.py", "size": 727, "textual": true, "content": "from typing import Dict\nimport numpy as np\n\nclass LinearRegressionModel:\n    def __init__(self, n_features: int):\n        self.params = {\n            \"w\": np.zeros((n_features, 1)),\n            \"b\": np.zeros((1, 1)),\n        }\n\n    def forward(self, X: np.ndarray) -> np.ndarray:\n        return X @ self.params[\"w\"] + self.params[\"b\"]\n\n    def loss_and_grads(self, X: np.ndarray, y: np.ndarray) -> (float, Dict[str, np.ndarray]):\n        # Mean squared error\n        y_pred = self.forward(X)\n        err = y_pred - y\n        loss = float((err ** 2).mean())\n        n = X.shape[0]\n        grads = {\n            \"w\": (2.0 / n) * X.T @ err,\n            \"b\": (2.0 / n) * err.sum(keepdims=True)\n        }\n        return loss, grads\n"}, {"path": "src/training/__init__.py", "size": 0, "textual": true, "content": ""}, {"path": "src/training/README.md", "size": 200, "textual": true, "content": "> Navigation: [Home](../../docs/index.md) \u00b7 [Site Map](../../docs/site-map.md) \u00b7 [Code Browser](../../docs/code-browser.md)\n\n# training\n\nTraining orchestration, loops, logging, and artifact writing."}, {"path": "src/training/train.py", "size": 3146, "textual": true, "content": "from pathlib import Path\nfrom typing import Dict, Any\nimport numpy as np\n\nfrom src.data.loaders import make_synthetic_linear\nfrom src.models.linear_regression import LinearRegressionModel\nfrom src.algorithms import SGD, SGDMomentum, Adam\n\nclass Trainer:\n    def __init__(self, cfg: Dict[str, Any], logger, out_dir: Path):\n        self.cfg = cfg\n        self.logger = logger\n        self.out_dir = out_dir\n\n        data_cfg = cfg.get(\"data\", {})\n        exp_cfg = cfg.get(\"experiment\", {})\n        seed = int(exp_cfg.get(\"seed\", 0))\n\n        X, y, w_true, b_true = make_synthetic_linear(\n            n_samples=int(data_cfg.get(\"n_samples\", 512)),\n            n_features=int(data_cfg.get(\"n_features\", 3)),\n            noise_std=float(data_cfg.get(\"noise_std\", 0.1)),\n            seed=seed,\n        )\n        self.X, self.y = X, y\n        self.w_true, self.b_true = w_true, b_true\n\n        model_cfg = cfg.get(\"model\", {})\n        if model_cfg.get(\"type\", \"linear_regression\") != \"linear_regression\":\n            raise ValueError(\"Only 'linear_regression' model is implemented in this scaffold.\")\n        self.model = LinearRegressionModel(n_features=int(data_cfg.get(\"n_features\", 3)))\n\n        tr = cfg.get(\"training\", {})\n        algo = tr.get(\"algorithm\", \"sgd\").lower()\n        lr = float(tr.get(\"lr\", 0.01))\n\n        if algo == \"sgd\":\n            self.opt = SGD(lr=lr)\n        elif algo == \"momentum\":\n            self.opt = SGDMomentum(lr=lr, momentum=float(tr.get(\"momentum\", 0.9)))\n        elif algo == \"adam\":\n            self.opt = Adam(lr=lr, beta1=float(tr.get(\"beta1\", 0.9)), beta2=float(tr.get(\"beta2\", 0.999)), eps=float(tr.get(\"eps\", 1e-8)))\n        else:\n            raise ValueError(f\"Unknown training.algorithm: {algo}\")\n\n        self.epochs = int(tr.get(\"epochs\", 100))\n        self.batch_size = int(tr.get(\"batch_size\", 64))\n\n    def batches(self, X, y, batch_size):\n        n = X.shape[0]\n        idx = np.arange(n)\n        np.random.shuffle(idx)\n        for i in range(0, n, batch_size):\n            j = idx[i:i+batch_size]\n            yield X[j], y[j]\n\n    def run(self):\n        self.logger.info(\"Starting training\")\n        history = []\n        for epoch in range(1, self.epochs + 1):\n            for Xb, yb in self.batches(self.X, self.y, self.batch_size):\n                loss, grads = self.model.loss_and_grads(Xb, yb)\n                self.model.params = self.opt.step(self.model.params, grads)\n\n            # full pass for logging\n            loss_full, _ = self.model.loss_and_grads(self.X, self.y)\n            history.append({\"epoch\": epoch, \"loss\": loss_full})\n            if epoch % max(1, self.epochs // 10) == 0 or epoch == 1:\n                self.logger.info(f\"epoch {epoch:3d} | loss={loss_full:.6f}\")\n\n        # Save run artifacts\n        np.savez(self.out_dir / \"artifacts.npz\", w=self.model.params[\"w\"], b=self.model.params[\"b\"], w_true=self.w_true, b_true=self.b_true)\n        with open(self.out_dir / \"history.json\", \"w\", encoding=\"utf-8\") as f:\n            import json\n            json.dump(history, f, indent=2)\n        self.logger.info(\"Training complete. Artifacts written to %s\", str(self.out_dir))\n"}, {"path": "src/training/legacy/grad_desc_prob.py", "size": 17522, "textual": true, "content": "# \"Soft Safe\" Gradient Descent Playground\n# ------------------------------------------------------------\n# This single Python cell gives you:\n#   \u2022 A smooth nonconvex loss on a 3\u2011torus (three circular dials).\n#   \u2022 Multiple optimizers: fixed-step GD, backtracking GD, momentum/Nesterov, Adam.\n#   \u2022 Options for learning\u2011rate schedules, noise, multi\u2011start, and diagnostics.\n#   \u2022 Heavy comments with the math, when to use which option, and why.\n#\n# RULES (for this environment):\n#   - Uses Matplotlib (no seaborn), one chart per figure, no custom colors/styles.\n#\n# You can scroll through the code comments as a mini\u2011tutorial.\n# At the bottom, we run a few demo experiments and draw simple plots.\n#\n# Notation:\n#   \u03b8 \u2208 T^3 where T is a circle; each \u03b8_i is wrapped to [-\u03c0, \u03c0).\n#   d(\u03b8, \u03b8*) = wrap(\u03b8 - \u03b8*) is the shortest signed angular difference.\n#\n# GLOBAL OBJECTS created:\n#   - theta_star: the hidden safe combination (randomized per run).\n#   - couplings: dict with c1,c2,c3 shaping nonconvexity.\n#\n# ------------------------------------------------------------\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(7)  # change seed to reshuffle the secret combo each run\n\n# --------------------------\n# Geometry utilities on the torus T^3\n# --------------------------\ndef wrap_angle(a):\n    \"\"\"\n    Wrap a (possibly vector) angle to [-\u03c0, \u03c0).\n    This defines the equivalence relation of angles modulo 2\u03c0 and\n    is essential for optimization on a torus rather than R^3.\n    \"\"\"\n    return (a + np.pi) % (2*np.pi) - np.pi\n\ndef angdiff(a, b):\n    \"\"\"\n    Shortest signed angular difference in [-\u03c0, \u03c0):\n    angdiff(a,b) = wrap(a - b).\n    \"\"\"\n    return wrap_angle(a - b)\n\ndef torus_distance(a, b):\n    \"\"\"\n    L2 distance on the torus: ||wrap(a-b)||_2.\n    This respects periodicity and is the relevant notion of \"closeness.\"\n    \"\"\"\n    d = angdiff(np.asarray(a), np.asarray(b))\n    return float(np.linalg.norm(d))\n\n# --------------------------\n# Loss: \"Soft Safe\" with gentle couplings\n# --------------------------\n# Global minimum is at \u03b8 = \u03b8* (mod 2\u03c0). The base term is convex on each angle,\n# but couplings introduce soft nonconvex structure and benign local minima.\ntheta_star = rng.uniform(-np.pi, np.pi, size=3)  # secret target\ncouplings = dict(c1=0.30, c2=0.20, c3=0.10)\n\ndef lock_loss(theta, theta_star=theta_star, couplings=couplings):\n    r\"\"\"\n    L(\u03b8) = \u03a3_i (1 - cos d_i)\n           + c1 * (1 - cos(d0 + 2 d1))\n           + c2 * (1 - cos(2 d1 - d2))\n           + c3 * (1 - cos(3 d0 - d1 + d2))\n    where d = wrap(\u03b8 - \u03b8*).\n\n    WHY THIS FORM:\n    - 1 - cos(d_i) has a unique minimum at d_i = 0, quadratic near 0:\n        1 - cos(d_i) \u2248 0.5 d_i^2.\n      Its gradient is sin(d_i). This creates a bowl around \u03b8*.\n    - Coupling terms are low\u2011weight perturbations that keep the same global\n      minimizer but create gentle local basins so GD has to think.\n    \"\"\"\n    theta = np.asarray(theta, dtype=float)\n    d = angdiff(theta, theta_star)\n    base = float(np.sum(1 - np.cos(d)))\n    c1, c2, c3 = couplings[\"c1\"], couplings[\"c2\"], couplings[\"c3\"]\n    t1 = 1 - np.cos(d[0] + 2*d[1])\n    t2 = 1 - np.cos(2*d[1] - d[2])\n    t3 = 1 - np.cos(3*d[0] - d[1] + d[2])\n    return base + c1*t1 + c2*t2 + c3*t3\n\ndef lock_grad(theta, theta_star=theta_star, couplings=couplings):\n    r\"\"\"\n    Analytic gradient \u2207L(\u03b8). Using d = wrap(\u03b8 - \u03b8*).\n    For a term 1 - cos(u), d/du = sin(u).\n    Chain rule gives \u2202/\u2202\u03b8_i [1 - cos(d_i)] = sin(d_i) * \u2202d_i/\u2202\u03b8_i.\n    Ignoring the measure\u2011zero wrap kinks, \u2202d_i/\u2202\u03b8_i = 1 almost everywhere.\n\n    Coupling terms:\n      t1 = 1 - cos(d0 + 2 d1)     \u21d2 \u2207 = sin(d0 + 2 d1) * [1, 2, 0]\n      t2 = 1 - cos(2 d1 - d2)     \u21d2 \u2207 = sin(2 d1 - d2) * [0, 2, -1]\n      t3 = 1 - cos(3 d0 - d1 + d2)\u21d2 \u2207 = sin(3 d0 - d1 + d2) * [3, -1, 1]\n\n    NOTE: On the torus, take a gradient step in R^3, then wrap.\n    \"\"\"\n    theta = np.asarray(theta, dtype=float)\n    d = angdiff(theta, theta_star)\n    g = np.zeros(3, dtype=float)\n\n    # base\n    g += np.sin(d)\n\n    c1, c2, c3 = couplings[\"c1\"], couplings[\"c2\"], couplings[\"c3\"]\n\n    s1 = np.sin(d[0] + 2*d[1])\n    g[0] += c1 * s1 * 1.0\n    g[1] += c1 * s1 * 2.0\n\n    s2 = np.sin(2*d[1] - d[2])\n    g[1] += c2 * s2 * 2.0\n    g[2] += c2 * s2 * (-1.0)\n\n    s3 = np.sin(3*d[0] - d[1] + d[2])\n    g[0] += c3 * s3 * 3.0\n    g[1] += c3 * s3 * (-1.0)\n    g[2] += c3 * s3 * 1.0\n\n    return g\n\n# --------------------------\n# Gradient checker (optional but educational)\n# --------------------------\ndef check_grad(f, grad, x, h=1e-5):\n    \"\"\"\n    Central finite\u2011difference gradient check on the torus.\n    Returns relative error ||g_fd - g|| / max(1, ||g||, ||g_fd||).\n\n    Use this once to sanity\u2011check your analytic gradient.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    g_fd = np.zeros_like(x)\n    for i in range(len(x)):\n        e = np.zeros_like(x); e[i] = 1.0\n        xph = wrap_angle(x + h*e)\n        xmh = wrap_angle(x - h*e)\n        g_fd[i] = (f(xph) - f(xmh)) / (2*h)\n    g = grad(x)\n    denom = max(1.0, float(np.linalg.norm(g)), float(np.linalg.norm(g_fd)))\n    rel = float(np.linalg.norm(g_fd - g)) / denom\n    return rel, g, g_fd\n\n# --------------------------\n# Learning\u2011rate schedules\n# --------------------------\ndef lr_constant(lr0):\n    \"\"\"Constant LR: \u03b7_k = lr0. Good when a line search is expensive and the landscape is tame.\"\"\"\n    def s(k): return lr0\n    return s\n\ndef lr_geometric(lr0, gamma=0.99):\n    \"\"\"Geometric decay: \u03b7_k = lr0 * \u03b3^k. Useful when you want coarse steps early, fine steps late.\"\"\"\n    def s(k): return lr0 * (gamma ** k)\n    return s\n\ndef lr_inverse_sqrt(lr0, warmup=10):\n    \"\"\"Inverse\u2011sqrt: \u03b7_k = lr0 / sqrt(k + warmup). A classic schedule with diminishing steps.\"\"\"\n    def s(k): return lr0 / math.sqrt(k + warmup)\n    return s\n\n# --------------------------\n# Core optimizers on the torus\n# --------------------------\ndef gd_fixed(f, grad, x0, lr_schedule, max_iter=2000, tol_g=1e-9, tol_x=1e-10,\n             noise_std=0.0, noise_every=None, callback=None):\n    r\"\"\"\n    Fixed\u2011step gradient descent on T^3.\n\n    Update:\n        g_k = \u2207f(x_k)\n        \u03b7_k = schedule(k)\n        x_{k+1} = wrap(x_k - \u03b7_k g_k)\n\n    Stopping:\n        ||g_k|| < tol_g   or   ||wrap(x_{k+1}-x_k)|| < tol_x   or   k hits max_iter.\n\n    Options:\n      - lr_schedule: one of the schedule factories above.\n      - noise_std / noise_every: add small Gaussian noise to escape shallow traps.\n        x_{k+1} \u2190 wrap(x_{k+1} + N(0, noise_std^2 I)) every 'noise_every' steps.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    xs = [x.copy()]; fs = [float(f(x))]\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = float(np.linalg.norm(g))\n        if gnorm < tol_g: break\n        step = lr_schedule(k)\n        xn = wrap_angle(x - step * g)\n        if noise_every is not None and ((k+1) % noise_every == 0) and noise_std > 0:\n            xn = wrap_angle(xn + rng.normal(0, noise_std, size=xn.shape))\n        xs.append(xn.copy()); fs.append(float(f(xn)))\n        if float(np.linalg.norm(angdiff(xn, x))) < tol_x: x = xn; break\n        x = xn\n    return x, np.array(xs), np.array(fs)\n\ndef gd_backtracking(f, grad, x0, lr_init=0.3, alpha=1e-4, beta=0.6, max_iter=2000,\n                    tol_g=1e-9, tol_x=1e-10, callback=None):\n    r\"\"\"\n    Backtracking line search (Armijo). Robust first choice if you're allergic to tuning.\n\n    Pick step \u03b7 starting from lr_init, shrink by \u03b2 \u2208 (0,1) until\n        f(x - \u03b7 g) \u2264 f(x) - \u03b1 \u03b7 ||g||^2\n    where \u03b1 \u2208 (0, 0.5) is the sufficient\u2011decrease parameter.\n\n    WHY:\n      - Automatic step control; works well when curvature varies across the space.\n      - More function evals per iteration, but fewer faceplants.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    xs = [x.copy()]; fx = float(f(x)); fs = [fx]\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = float(np.linalg.norm(g))\n        if gnorm < tol_g: break\n        step = lr_init\n        # Candidate\n        xn = wrap_angle(x - step * g)\n        fn = float(f(xn))\n        # Armijo backtracking\n        iters = 0\n        while fn > fx - alpha * step * (gnorm**2):\n            step *= beta\n            xn = wrap_angle(x - step * g)\n            fn = float(f(xn))\n            iters += 1\n            if iters > 60:\n                break\n        xs.append(xn.copy()); fs.append(fn)\n        if float(np.linalg.norm(angdiff(xn, x))) < tol_x:\n            x = xn; fx = fn; break\n        x = xn; fx = fn\n    return x, np.array(xs), np.array(fs)\n\ndef gd_momentum(f, grad, x0, lr=0.2, mu=0.9, nesterov=False, max_iter=2000,\n                tol_g=1e-9, tol_x=1e-10, callback=None):\n    r\"\"\"\n    Momentum / Nesterov on T^3.\n\n    Classical momentum:\n        v_{k+1} = \u03bc v_k - \u03b7 \u2207f(x_k)\n        x_{k+1} = wrap(x_k + v_{k+1})\n\n    Nesterov (look\u2011ahead) gradient:\n        g_k = \u2207f(x_k + \u03bc v_k)\n        v_{k+1} = \u03bc v_k - \u03b7 g_k\n        x_{k+1} = wrap(x_k + v_{k+1})\n\n    WHEN:\n      - Use when the landscape has long, gently sloped valleys; momentum accelerates.\n      - Nesterov can reduce overshoot by peeking ahead.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    v = np.zeros_like(x)\n    xs = [x.copy()]; fs = [float(f(x))]\n    for k in range(max_iter):\n        if nesterov:\n            g = grad(wrap_angle(x + mu * v))\n        else:\n            g = grad(x)\n        gnorm = float(np.linalg.norm(g))\n        if gnorm < tol_g: break\n        v = mu * v - lr * g\n        xn = wrap_angle(x + v)\n        xs.append(xn.copy()); fs.append(float(f(xn)))\n        if float(np.linalg.norm(angdiff(xn, x))) < tol_x:\n            x = xn; break\n        x = xn\n    return x, np.array(xs), np.array(fs)\n\ndef adam_torus(f, grad, x0, lr=0.1, beta1=0.9, beta2=0.999, eps=1e-8,\n               max_iter=4000, tol_g=1e-9, tol_x=1e-10, callback=None):\n    r\"\"\"\n    Adam optimizer on the torus.\n\n    m_{k+1} = \u03b21 m_k + (1-\u03b21) g_k\n    v_{k+1} = \u03b22 v_k + (1-\u03b22) g_k^2\n    m\u0302 = m_{k+1} / (1-\u03b21^{k+1})\n    v\u0302 = v_{k+1} / (1-\u03b22^{k+1})\n    x_{k+1} = wrap(x_k - lr * m\u0302 / (sqrt(v\u0302) + eps))\n\n    WHEN:\n      - Coordinates have different curvature/scales. Adam adaptively rescales steps.\n      - Often converges with minimal tuning; beware too\u2011large lr.\n    \"\"\"\n    x = np.asarray(x0, dtype=float)\n    m = np.zeros_like(x)\n    v = np.zeros_like(x)\n    xs = [x.copy()]; fs = [float(f(x))]\n    for k in range(max_iter):\n        g = grad(x)\n        gnorm = float(np.linalg.norm(g))\n        if gnorm < tol_g: break\n        m = beta1 * m + (1 - beta1) * g\n        v = beta2 * v + (1 - beta2) * (g * g)\n        mhat = m / (1 - beta1 ** (k + 1))\n        vhat = v / (1 - beta2 ** (k + 1))\n        step = lr * mhat / (np.sqrt(vhat) + eps)\n        xn = wrap_angle(x - step)\n        xs.append(xn.copy()); fs.append(float(f(xn)))\n        if float(np.linalg.norm(angdiff(xn, x))) < tol_x:\n            x = xn; break\n        x = xn\n    return x, np.array(xs), np.array(fs)\n\n# --------------------------\n# Multi\u2011start driver and success metrics\n# --------------------------\ndef solve_multistart(optimizer_fn, f, grad, n_starts=40, **opt_kwargs):\n    \"\"\"\n    Try many random initializations; keep best result and compute success rate.\n    A run is 'successful' if it recovers \u03b8* within tight torus distance and loss.\n    \"\"\"\n    best = None\n    successes = 0\n    paths, losses = None, None\n    for _ in range(n_starts):\n        x0 = rng.uniform(-np.pi, np.pi, size=3)\n        x, xs, fs = optimizer_fn(f, grad, x0, **opt_kwargs)\n        val = f(x)\n        dist = torus_distance(x, theta_star)\n        ok = (dist < 1e-3) and (val < 1e-8)\n        if ok: successes += 1\n        if (best is None) or (val < best[1]):\n            best = (x, val, x0)\n            paths, losses = xs, fs\n    success_rate = successes / n_starts\n    return dict(best_x=best[0], best_val=best[1], best_start=best[2],\n                best_path=paths, best_losses=losses, success_rate=success_rate)\n\n# --------------------------\n# 1) Sanity check the gradient\n# --------------------------\nx_check = rng.uniform(-np.pi, np.pi, size=3)\nrel_err, g_analytic, g_fd = check_grad(lock_loss, lock_grad, x_check)\nprint(\"Gradient check at random point\")\nprint(\"  point:\", np.round(x_check, 4))\nprint(\"  rel error:\", f\"{rel_err:.3e}\", \"(<1e-6 is excellent; <1e-4 is fine for this problem)\")\n\n# --------------------------\n# 2) Single\u2011run comparisons from the same start\n# --------------------------\nx0 = rng.uniform(-np.pi, np.pi, size=3)\nprint(\"\\nSingle\u2011run comparisons from a shared start:\")\nprint(\"  secret \u03b8*:\", np.round(theta_star, 6))\nprint(\"  start \u03b80 :\", np.round(x0, 6))\n\n# Fixed\u2011step with geometric decay\nx_fixed, path_fixed, loss_fixed = gd_fixed(\n    lock_loss, lock_grad, x0,\n    lr_schedule=lr_geometric(lr0=0.35, gamma=0.985),\n    max_iter=800\n)\n\n# Backtracking\nx_bt, path_bt, loss_bt = gd_backtracking(\n    lock_loss, lock_grad, x0,\n    lr_init=0.35, alpha=1e-4, beta=0.6, max_iter=400\n)\n\n# Momentum (Nesterov on)\nx_mom, path_mom, loss_mom = gd_momentum(\n    lock_loss, lock_grad, x0,\n    lr=0.22, mu=0.9, nesterov=True, max_iter=800\n)\n\n# Adam\nx_adam, path_adam, loss_adam = adam_torus(\n    lock_loss, lock_grad, x0,\n    lr=0.12, max_iter=1200\n)\n\ndef report(name, x_final, losses):\n    dist = torus_distance(x_final, theta_star)\n    print(f\"  {name:12s} | iters={len(losses)-1:4d}  loss={losses[-1]:.3e}  dist={dist:.3e}\")\n\nreport(\"fixed\u2011step\", x_fixed, loss_fixed)\nreport(\"backtracking\", x_bt, loss_bt)\nreport(\"momentum+NAG\", x_mom, loss_mom)\nreport(\"adam\", x_adam, loss_adam)\n\n# Plot loss curves (one figure per optimizer)\nplt.figure()\nplt.plot(np.arange(len(loss_fixed)), loss_fixed)\nplt.title(\"Fixed\u2011step GD: loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\nplt.figure()\nplt.plot(np.arange(len(loss_bt)), loss_bt)\nplt.title(\"Backtracking GD: loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\nplt.figure()\nplt.plot(np.arange(len(loss_mom)), loss_mom)\nplt.title(\"Momentum (Nesterov): loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\nplt.figure()\nplt.plot(np.arange(len(loss_adam)), loss_adam)\nplt.title(\"Adam: loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\n# --------------------------\n# 3) Success probability vs learning rate (fixed\u2011step)\n# --------------------------\nlrs = [0.05, 0.1, 0.18, 0.25, 0.35, 0.5]\nsuccess = []\nfor lr0 in lrs:\n    res = solve_multistart(\n        gd_fixed, lock_loss, lock_grad, n_starts=30,\n        lr_schedule=lr_constant(lr0), max_iter=800\n    )\n    success.append(res[\"success_rate\"])\n\nplt.figure()\nplt.plot(lrs, success, marker='o')\nplt.title(\"Fixed\u2011step GD: success rate vs learning rate\")\nplt.xlabel(\"learning rate\")\nplt.ylabel(\"success fraction\")\nplt.ylim(0, 1.05)\nplt.show()\n\n# --------------------------\n# 4) Multi\u2011start with backtracking (robust baseline)\n# --------------------------\nres_bt = solve_multistart(\n    gd_backtracking, lock_loss, lock_grad, n_starts=60,\n    lr_init=0.35, alpha=1e-4, beta=0.6, max_iter=600\n)\nx_best = res_bt[\"best_x\"]\ndist_best = torus_distance(x_best, theta_star)\nprint(\"\\nMulti\u2011start (backtracking):\")\nprint(\"  best start:\", np.round(res_bt[\"best_start\"], 6))\nprint(\"  recovered :\", np.round(x_best, 6))\nprint(\"  distance to \u03b8*:\", f\"{dist_best:.3e}\")\nprint(\"  final loss:\", f\"{res_bt['best_val']:.3e}\")\nprint(\"  success rate:\", f\"{res_bt['success_rate']:.2f} over 60 starts\")\n\nplt.figure()\nplt.plot(np.arange(len(res_bt[\"best_losses\"])), res_bt[\"best_losses\"])\nplt.title(\"Best run (backtracking): loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()\n\n# --------------------------\n# 5) Visual intuition: 2D contour slice with a path overlay\n#    Fix \u03b81 and visualize L(\u03b80, \u03b82 | \u03b81=const). Overlay the best backtracking path.\n# --------------------------\ntheta1_fixed = float(res_bt[\"best_path\"][0][1])  # take initial \u03b81 for the best run\ngrid = 150\nt0 = np.linspace(-np.pi, np.pi, grid)\nt2 = np.linspace(-np.pi, np.pi, grid)\nT0, T2 = np.meshgrid(t0, t2, indexing='xy')\nZ = np.zeros_like(T0)\n\n# Build the slice; this is compute heavy but fine at 150x150\nfor i in range(grid):\n    for j in range(grid):\n        th = np.array([T0[i, j], theta1_fixed, T2[i, j]])\n        Z[i, j] = lock_loss(th)\n\nplt.figure()\n# contourf uses a default colormap; we don't specify any styles.\ncs = plt.contourf(T0, T2, Z, levels=25)\nplt.colorbar()\nplt.title(r\"Contour slice: $L(\\theta_0,\\theta_2\\,|\\,\\theta_1=\\mathrm{const})$\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_2$\")\n\n# Overlay the best path (project to this slice by taking its \u03b80, \u03b82)\np = np.array(res_bt[\"best_path\"])\nplt.plot(p[:,0], p[:,2], marker='o', linewidth=1)\nplt.show()\n\n# --------------------------\n# 6) Optional: demonstrate noise annealing escape\n# --------------------------\n# We'll run a fixed\u2011step with tiny periodic noise; sometimes this bumps you out of a shallow trap.\nx0_noise = rng.uniform(-np.pi, np.pi, size=3)\nx_noise, path_noise, loss_noise = gd_fixed(\n    lock_loss, lock_grad, x0_noise,\n    lr_schedule=lr_constant(0.22),\n    max_iter=800,\n    noise_std=0.02, noise_every=10\n)\nprint(\"\\nNoise\u2011annealed fixed\u2011step example:\")\nprint(\"  start:\", np.round(x0_noise, 6))\nprint(\"  end  :\", np.round(x_noise, 6))\nprint(\"  dist :\", f\"{torus_distance(x_noise, theta_star):.3e}\")\nplt.figure()\nplt.plot(np.arange(len(loss_noise)), loss_noise)\nplt.title(\"Fixed\u2011step with periodic noise: loss vs iteration\")\nplt.xlabel(\"iteration\"); plt.ylabel(\"loss\")\nplt.show()"}, {"path": "src/training/legacy/README.md", "size": 136, "textual": true, "content": "> Navigation: [Home](../../../docs/index.md) \u00b7 [Site Map](../../../docs/site-map.md) \u00b7 [Code Browser](../../../docs/code-browser.md)\n\n"}, {"path": "src/utils/logging.py", "size": 908, "textual": true, "content": "import logging\nfrom pathlib import Path\nfrom typing import Optional\n\ndef get_logger(name: str = \"app\", level: str = \"INFO\", log_dir: Optional[str] = None) -> logging.Logger:\n    logger = logging.getLogger(name)\n    if logger.handlers:\n        return logger  # avoid duplicate handlers in notebooks\n\n    numeric_level = getattr(logging, level.upper(), logging.INFO)\n    logger.setLevel(numeric_level)\n\n    fmt = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\")\n\n    stream = logging.StreamHandler()\n    stream.setFormatter(fmt)\n    stream.setLevel(numeric_level)\n    logger.addHandler(stream)\n\n    if log_dir:\n        Path(log_dir).mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(Path(log_dir) / f\"{name}.log\")\n        file_handler.setFormatter(fmt)\n        file_handler.setLevel(numeric_level)\n        logger.addHandler(file_handler)\n\n    return logger\n"}, {"path": "src/utils/config.py", "size": 700, "textual": true, "content": "from pathlib import Path\nfrom typing import Any, Dict\n\ndef load_config(path: str) -> Dict[str, Any]:\n    \"\"\"Load a YAML config. Falls back with a clear message if PyYAML isn't installed.\"\"\"\n    try:\n        import yaml  # type: ignore\n    except Exception as e:\n        raise RuntimeError(\n            \"PyYAML is required to load configs. Install with `pip install pyyaml`.\"\n        ) from e\n\n    p = Path(path)\n    if not p.exists():\n        raise FileNotFoundError(f\"Config file not found: {path}\")\n    with p.open(\"r\", encoding=\"utf-8\") as f:\n        data = yaml.safe_load(f)\n    if not isinstance(data, dict):\n        raise ValueError(\"Top-level YAML must be a mapping (dict).\" )\n    return data\n"}, {"path": "src/utils/__init__.py", "size": 0, "textual": true, "content": ""}, {"path": "src/utils/README.md", "size": 176, "textual": true, "content": "> Navigation: [Home](../../docs/index.md) \u00b7 [Site Map](../../docs/site-map.md) \u00b7 [Code Browser](../../docs/code-browser.md)\n\n# utils\n\nConfiguration loading and logging setup."}, {"path": "tests/test_optimizers.py", "size": 815, "textual": true, "content": "import numpy as np\nfrom src.algorithms import SGD, SGDMomentum, Adam\n\ndef quad_loss_and_grad(x):\n    # f(x) = (x-3)^2\n    f = float((x - 3.0) ** 2)\n    g = 2.0 * (x - 3.0)\n    return f, g\n\ndef run_optimizer(opt, steps=200, lr=0.05):\n    x = 0.0\n    hist = []\n    for _ in range(steps):\n        f, g = quad_loss_and_grad(x)\n        x = opt.step({\"x\": np.array([[x]])}, {\"x\": np.array([[g]])})[\"x\"][0,0]\n        hist.append(f)\n    return hist\n\ndef test_sgd_decreases_loss():\n    opt = SGD(lr=0.05)\n    hist = run_optimizer(opt)\n    assert hist[-1] < hist[0]\n\ndef test_momentum_decreases_loss():\n    opt = SGDMomentum(lr=0.05, momentum=0.9)\n    hist = run_optimizer(opt)\n    assert hist[-1] < hist[0]\n\ndef test_adam_decreases_loss():\n    opt = Adam(lr=0.05)\n    hist = run_optimizer(opt)\n    assert hist[-1] < hist[0]\n"}, {"path": "tests/back-arrow.png", "size": 47253, "textual": false, "content": null}, {"path": "tests/README.md", "size": 160, "textual": true, "content": "> Navigation: [Home](../docs/index.md) \u00b7 [Site Map](../docs/site-map.md) \u00b7 [Code Browser](../docs/code-browser.md)\n\n# tests\n\nUnit tests kept tidy and minimal."}];
      function hBytes(n){const u=['B','KB','MB','GB'];let i=0,q=n;while(q>=1024 && i<u.length-1){q/=1024;i++;}return q.toFixed(1)+' '+u[i];}
      function escapeHtml(s){return s.replace(/&/g,'&amp;').replace(/</g,'&lt;').replace(/>/g,'&gt;');}
      function buildList(){
        const el=document.getElementById('fileList'); const frag=document.createDocumentFragment();
        const dirs=new Map(); for(const item of manifest){const parts=item.path.split('/');const file=parts.pop();const dir=parts.join('/')||'.'; if(!dirs.has(dir))dirs.set(dir,[]); dirs.get(dir).push(item);}
        const sorted=[...dirs.keys()].sort((a,b)=>a.localeCompare(b));
        for(const dir of sorted){const d=document.createElement('details');d.className='dir';d.open=dir==='.';const s=document.createElement('summary');s.textContent=dir==='.'?'root':dir;d.appendChild(s);const ul=document.createElement('ul');
          for(const item of dirs.get(dir)){const li=document.createElement('li');const a=document.createElement('a');a.href='#';a.textContent=item.path.split('/').pop()+' Â· '+hBytes(item.size);a.addEventListener('click',e=>{e.preventDefault();showFile(item);});li.appendChild(a);ul.appendChild(li);}
          d.appendChild(ul); frag.appendChild(d);}
        el.appendChild(frag);
      }
      function showFile(item){const v=document.getElementById('viewer'); if(!item.textual){v.innerHTML=`<div class="card"><h2>${item.path}</h2><p>This file is binary or large. <a href="project/${item.path}" download>Download it</a>.</p></div>`; return;}
        v.innerHTML=`<div class="card"><div class="file-head"><h2>${item.path}</h2><a class="btn" href="project/${item.path}" download>Download</a></div><pre class="code"><code>${escapeHtml(item.content)}</code></pre></div>`; }
      document.addEventListener('DOMContentLoaded',()=>{buildList();});
    </script>
    <script src="../assets/script.js"></script>
  </body>
</html>
